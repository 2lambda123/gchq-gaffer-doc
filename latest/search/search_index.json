{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gaffer 2","text":"<p>These Docs are a work in progress for Gaffer v2, the docs for Gaffer v1 can be found here.</p> <p>For information and migration steps for Deprecated classes which have been removed in Gaffer 2 alpha 1, please see the deprecations page.</p> <p>For a summary of the current Gaffer 2 alpha roadmap, including migration steps, please see the Gaffer 2 Changelist.</p> <p>For information on logging in Gaffer and historic use of Log4j, please see this page.</p>"},{"location":"#licence","title":"Licence","text":"<p>Gaffer is licensed under the Apache 2 licence and is covered by Crown Copyright.</p>"},{"location":"ImportCsv/","title":"ImportCsv","text":"<p>The ImportCsv operation allows a user to import data into a running Gaffer instance by simply providing the path to a CSV file. The data within the CSV must adhere to the openCypher format.  As described in the previous link there exists small differences in the headers used between Neptune and neo4j,  the ImportCsv operation handles both, it also handles both entities and edges specified within the same file.</p>"},{"location":"ImportCsv/#example","title":"Example","text":"<p>The following example shows how to load a CSV using the openCypher CSV format into a Gaffer graph. This step requires a running Gaffer instance.</p>"},{"location":"ImportCsv/#opencypher-load-format","title":"openCypher load format","text":"<pre><code>:ID,  name:String,  age:Int,  lang:String,  :LABEL,  :START_ID,  :END_ID,   :TYPE,  weight:Double\n v1,      \"marko\",       29,             ,  person,           ,         ,        ,\n v2,        \"lop\",         ,       \"java\",software,           ,         ,        ,\n e1,             ,         ,             ,        ,         v1,       v2, created,            0.4\n</code></pre> <p>From the rest-api execute the following operation from POST /graph/operation/execute. As the user you must provide the file path to the CSV file containing your data.</p>"},{"location":"ImportCsv/#operation-to-execute","title":"Operation to Execute","text":"<pre><code>{\n    \"class\": \"uk.gov.gchq.gaffer.operation.impl.add.ImportCsv\",\n    \"filename\": \"path/to/data.csv\"\n}\n</code></pre>"},{"location":"accumulo-kerberos/","title":"Accumulo Kerberos Support","text":"<p>This page contains information on Kerberos Authentication support for Gaffer's Accumulo Store. This functionality was introduced in version <code>2.0.0-alpha-0.3.1</code> of Gaffer.</p>"},{"location":"accumulo-kerberos/#using-the-accumulo-store-with-kerberos","title":"Using the Accumulo Store with Kerberos","text":""},{"location":"accumulo-kerberos/#prerequisites","title":"Prerequisites","text":"<p>To use Gaffer's Accumulo Store with Kerberos authentication:</p> <ul> <li>The Accumulo cluster to connect with must be correctly configured to use Kerberos.</li> <li>A principal for the system/host Gaffer will be running on must be created in the Key Distribution Center (KDC) database.</li> <li>The Gaffer principal should use the standard <code>primary/instance@realm</code> format. Using principals without an instance qualification has not been tested.</li> <li>A keytab for the Gaffer principal must be created and transferred to the Gaffer host.</li> <li>The Gaffer principle must have been added as an Accumulo user with suitable permissions granted.</li> <li>Kerberos client utilities should be installed on the host and <code>krb5.conf</code> must be correctly configured.</li> <li>An Accumulo client configuration should be available on the host and contain the correct options to enable Kerberos.</li> <li>The Gaffer store.properties should state that Kerberos is to be used, specify the principal name and the keytab path.</li> </ul> <p>The sections below cover some of these points in more detail.</p>"},{"location":"accumulo-kerberos/#accumulo-user-for-gaffer","title":"Accumulo user for Gaffer","text":"<p>When Kerberos is used with Accumulo, any client with a principal can connect without requiring an Accumulo user to have been created previously. This works by creating an Accumulo user automatically when a new client connects. These users are not granted any permissions.</p> <p>Users can still be created manually via the Accumulo shell, with Gaffer's full principal (with all components) given as the username. Permissions to create and read tables can then be granted to this user. If this isn't done, Accumulo will create the user automatically when Gaffer first connects. In this case Gaffer will fail to start as the required permissions will not have been granted - they can then be granted via the shell and Gaffer restarted.</p>"},{"location":"accumulo-kerberos/#accumulo-client-configuration","title":"Accumulo Client configuration","text":"<p>Depending on the version of Accumulo used, an <code>accumulo-client.properties</code> (2.x) or <code>client.conf</code> (1.x) must be populated as described in the respective version of the Accumulo documentation. The only value which needs to be altered is the Kerberos server primary. This should reflect the primary part of the principals used by the Accumulo cluster.</p> <p>The location of this config file can be specified using the <code>ACCUMULO_CLIENT_CONF_PATH</code> environment variable. If this is not set, then default paths will be checked.</p> <p>Other than this file, Accumulo libraries and configuration files do not need to be installed on the Gaffer host. </p>"},{"location":"accumulo-kerberos/#gaffer-storeproperties-configuration","title":"Gaffer <code>store.properties</code> configuration","text":"<p>In addition to the usual Accumulo Store settings, these extra options must be specified for Kerberos: <pre><code>accumulo.kerberos.enable=true\naccumulo.kerberos.principal=gaffer/host.domain@REALM.NAME\naccumulo.kerberos.keytab=/gaffer/config/gaffer.keytab\n</code></pre> The <code>accumulo.username</code> and <code>accumulo.password</code> values do not need to be set and are ignored when <code>accumulo.kerberos.enable</code> is true.</p> <p>The <code>kinit</code> Kerberos command does not need to be used, although it might be useful for ensuring the client principal works correctly. All Kerberos ticket management, renewal and re-login is handled automatically.</p>"},{"location":"accumulo-kerberos/#specifying-a-different-krb5conf","title":"Specifying a different <code>krb5.conf</code>","text":"<p>If the <code>krb5.conf</code> in the default system location is not suitable, or if it's stored in a non-standard location, then  custom a custom <code>krb5.conf</code> location can be specified when starting Gaffer by setting the system property value <code>java.security.krb5.conf</code>. The simplest way to do this is by using the option flag <code>-Djava.security.krb5.conf=/my/path/to/krb5.conf</code> when launching the Gaffer JAR.</p>"},{"location":"accumulo-kerberos/#federation-considerations","title":"Federation Considerations","text":"<p>Due to the way Kerberos is implemented in Accumulo, it is not possible for Gaffer to use multiple principals at the same time. For the <code>FederatedStore</code>, this prevents adding graphs which are on different Accumulo clusters, if those clusters require different principals. In practice this is unlikely to be a problem, as different Accumulo clusters would only need separate client principals if they were on separate Kerberos Realms or using different KDCs.</p> <p>This only impacts Accumulo clusters which require Kerberos. It doesn't impact on adding graphs which are stored in clusters using basic authentication and not Kerberos. Nor does it affect adding graphs from a Kerberos cluster and also adding graphs from a non Kerberos cluster in the same <code>FederatedStore</code>.</p> <p>If this limitation is a problem, it can be worked around by running additional Gaffer instances and connecting to them using a <code>ProxyStore</code> in the <code>FederatedStore</code>, rather than connecting directly using an <code>AccumuloStore</code>.</p>"},{"location":"accumulo-kerberos/#hdfs-considerations","title":"HDFS Considerations","text":"<p>When using the <code>AddElementsFromHdfs</code> operation Gaffer acts as a HDFS client. When Kerberos is used (Hadoop Secure Mode), HDFS clients must have native libraries installed and configured correctly; else Hadoop will raise a Runtime Exception stating that \"Secure IO is not possible without native code extensions\".</p> <p>The HDFS client also requires the Hadoop configuration files <code>core-site.xml</code> and <code>hdfs-site.xml</code> to both be present and configured as below. The location of these files can be specified using the <code>HADOOP_CONF_DIR</code> environment variable.</p> <pre><code>&lt;!--Properties in core-site.xml--&gt;\n&lt;property&gt;\n&lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n&lt;value&gt;kerberos&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n&lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>In particular, <code>hdfs-site.xml</code> requires the <code>yarn.resourcemanager.principal</code> property to be set to the HDFS client principal - should be the same one as in the Gaffer Store properties. If this is missing Hadoop will fail to connect and raise an IO Exception with \"Can't get Master Kerberos principal for use as renewer\".</p> <pre><code>&lt;!--Properties in hdfs-site.xml--&gt;\n&lt;property&gt;\n&lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n&lt;value&gt;primary/instance@realm&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Note that the <code>core-site.xml</code> and <code>hdfs-site.xml</code> files are only required if <code>AddElementsFromHdfs</code> is going to be used. For Accumulo connections the Hadoop properties (from <code>core-site.xml</code>) used for enabling Kerberos are set automatically in Gaffer's connection code.</p>"},{"location":"accumulo-kerberos/#spark-accumulo-library","title":"Spark Accumulo Library","text":"<p>The Spark Accumulo Library has not yet been updated to support Kerberos. This prevents Spark Operations from being used with an <code>AccumuloStore</code> which has Kerberos authentication enabled. It is on the backlog for support to be added in future.</p>"},{"location":"accumulo-kerberos/#troubleshooting","title":"Troubleshooting","text":"<p>Kerberos is not easy to configure and familiarity with Kerberos concepts is recommended. There are some useful links to introductory information in the Accumulo Kerberos docs.</p> <p>Improperly configured DNS will cause problems with Kerberos. Ensure all hostnames used in Principals resolved correctly, include reverse lookup. Due to how the system's hostname is used by the Hadoop Kerberos libraries, a mismatch between the configured hostname and the hostname resolved by a reverse lookup can prevent authentication from working correctly.</p> <p>Various environment variables can be set for debugging Kerberos, see the Hadoop docs for more information. These variables are applicable to Accumulo (see docs) because its Kerberos implementation uses Hadoop libraries. The Gaffer logging level (set in <code>log4.xml</code>) should be increased to at least <code>INFO</code> when using these environment variables.</p>"},{"location":"accumulo-migration/","title":"Accumulo Migration","text":"<p>This page contains information on changes to the Accumulo/Hadoop versions supported by Gaffer and how to continue using the previously supported versions.</p>"},{"location":"accumulo-migration/#accumulo-2-hadoop-3-become-default-versions","title":"Accumulo 2 &amp; Hadoop 3 become default versions","text":"<p>From the <code>2.0.0-alpha-3</code> release of Gaffer, the default version of Accumulo has been upgraded to Accumulo 2.0.1. Hadoop has also been upgraded to the latest version (currently 3.3.3). This is because Hadoop 2.x is not compatible with Accumulo 2.</p>"},{"location":"accumulo-migration/#retained-support-for-accumulo-1-hadoop-2","title":"Retained support for Accumulo 1 &amp; Hadoop 2","text":"<p>Support for certain versions of Accumulo 1 and Hadoop 2 (specifically 1.9.3 &amp; 2.6.5) has been retained and can be enabled by using a Maven profile when building from source. This facilitates testing with these versions and creates shaded JARs (e.g. spring-rest exec, accumulo-store iterators) with the appropriate versions of supporting libraries. As described in the source docs, other versions of Accumulo 1.x and Hadoop 2.x might also work.</p>"},{"location":"accumulo-migration/#building-gaffer-with-the-legacy-profile","title":"Building Gaffer with the 'legacy' profile","text":"<p>To build Gaffer using Accumulo 1.9.3 and Hadoop 2.6.5, the 'legacy' Maven profile needs to be used. This is enabled by supplying <code>-Dlegacy=true</code> as an extra argument at the command line when running Maven. For example, <code>mvn clean install -Pcoverage -Dlegacy=true</code> will perform a full build/test of Gaffer with this profile enabled. Java 11 cannot be used with this profile because only Hadoop 3.3.0 and higher support it.</p> <p>With the 'legacy' Maven profile active, the filenames of all shaded JARs produced are appended with <code>-legacy</code>. This is to differentiate them from the default shaded JARs which contain different libraries and different library versions. A default Gaffer Accumulo REST API JAR will not work with an Accumulo 1 cluster, and the 'legacy' version will not work with Accumulo 2.</p>"},{"location":"accumulo-migration/#migrating-from-accumulo-1-to-2","title":"Migrating from Accumulo 1 to 2","text":"<p>See the Accumulo documentation for guidance on upgrading from Accumulo 1 to 2. Of particular significance is the deprecation of the dynamic reloading classpath directory functionality in Accumulo 2. This affects where and how the Gaffer iterators JAR can be installed. See the Accumulo store documentation for these installation details.</p> <p>Otherwise, no Accumulo specific Gaffer configuration needs to be changed and migrating from Accumulo 1 to 2 should be as simple as swapping the Gaffer dependency versions/JARs, although this has not been actively tested.</p>"},{"location":"changelist/","title":"Gaffer 2 Changelist","text":"<p>Below is a summary of intended changes that will be made in Gaffer version 2.  Note: this represents the current roadmap which is not final (also available on GitHub) but the features may change.</p>"},{"location":"changelist/#alpha-1-released","title":"Alpha 1 | released","text":""},{"location":"changelist/#removal-of-deprecated-code","title":"Removal of Deprecated code","text":"<p>All of Gaffer 1's deprecated code has been removed. Please see the deprecations page for full details.</p>"},{"location":"changelist/#removal-of-hbase-and-parquet-stores","title":"Removal of HBase and Parquet stores","text":"<p>The HBase and Parquet stores have been removed from Gaffer in version 2. We made posts for both the HBase and Parquet stores to understand the levels of usage. It was then decided to remove both stores as this would make introducing various improvements easier in the long term. HBase and Parquet remain available in Gaffer version 1. In the future, they could be reimplemented for Gaffer 2, though we do not plan to currently.</p>"},{"location":"changelist/#alpha-2-released","title":"Alpha 2 | released","text":""},{"location":"changelist/#dependency-upgrades","title":"Dependency Upgrades","text":"<p>Dependencies have been updated, where possible to the latest version, removing vulnerabilities. Please see the dependencies page for full details.</p>"},{"location":"changelist/#gaffer-now-builds-with-java-8-and-java-11","title":"Gaffer now builds with Java 8 and Java 11","text":"<p>There is now a maven profile that will swap dependency versions so you can build Gaffer with Java 11. The code has also been updated to build with both Java versions.</p>"},{"location":"changelist/#removal-of-closeableiterable","title":"Removal of CloseableIterable","text":"<p>The CloseableIterable class has been removed so Operations like GetAllElements now return an Iterable instead, but the result still implements Closeable.</p>"},{"location":"changelist/#known-issues","title":"Known issues","text":"<p>The Road Traffic example REST has a bug in this release that means the example data isn't loaded in. This means that the gafferpy tests will fail as there is no data, but it still works.</p>"},{"location":"changelist/#alpha-3-released","title":"Alpha 3 | released","text":""},{"location":"changelist/#fixes-improvements","title":"Fixes &amp; Improvements","text":"<p>The Spring REST Swagger UI and Road Traffic example REST have been fixed and versions of Jersey and Jackson have both been updated.</p>"},{"location":"changelist/#accumulo-2-support","title":"Accumulo 2 Support","text":"<p>The Accumulo store now supports Accumulo 2 and Hadoop 3 by default, with support for Accumulo 1 and Hadoop 2 retained. See the Accumulo Migration page for more information about this change.</p>"},{"location":"changelist/#alpha-31-released","title":"Alpha 3.1 | released","text":""},{"location":"changelist/#csv-export","title":"CSV Export","text":"<p>Basic support for importing and exporting OpenCypher format CSVs has been added.</p>"},{"location":"changelist/#accumulo-kerberos-authentication-support","title":"Accumulo Kerberos Authentication Support","text":"<p>The Accumulo store now supports authenticating to Accumulo and HDFS using Kerberos, in addition to username/password. For more information, see the Kerberos support page</p>"},{"location":"changelist/#alpha-4-released","title":"Alpha 4 | released","text":""},{"location":"changelist/#federated-store-improvements","title":"Federated Store Improvements","text":"<p>The Federated Operation was added to improve flexibility of using a Federated Store. See the Federated Store Changes page.</p>"},{"location":"changelist/#future-alphas","title":"Future Alphas","text":""},{"location":"changelist/#named-operation-improvements","title":"Named Operation Improvements","text":"<p>Some changes and improvements to Named Operations are planned. Full details TBD.</p>"},{"location":"dependencies/","title":"Dependency Upgrades","text":"<p>This page lists the dependencies that have been upgraded as part of Gaffer 2.</p> <ul> <li>Assertj: 3.20.2 -&gt; 3.22.0</li> <li>Junit5: 5.6.0 -&gt; 5.8.2</li> <li>Mockito: 3.3.3 -&gt; 4.3.1</li> <li>Slf4j: 1.7.25 -&gt; 1.7.36</li> <li>Log4j: 1.2.17 -&gt; Reload4j: 1.2.18.3</li> <li>Koryphe: 1.14.0 -&gt; 2.1.0</li> <li>Avro: 1.7.7 -&gt; 1.8.2</li> <li>Jackson: 2.6.5 -&gt; 2.12.6</li> <li>Hazelcast: 3.8 -&gt; 5.1</li> <li>Spring Boot: 1.3.2 -&gt; 2.5.12</li> <li>Spring API Swagger: 2.6.0 -&gt; 3.0.0</li> <li>Commons-codec: 1.6 -&gt; 1.15</li> <li>Commons-io: 2.7 -&gt; 2.11.0</li> <li>Commons-lang: 3.3.2 -&gt; 3.12.0</li> <li>Commons-logging: 1.1.3 -&gt; 1.2</li> <li>Commons-math: 2.1 -&gt; 2.2</li> <li>Commons-math3: 3.4.1 -&gt; 3.6.1</li> <li>Commons-csv: 1.4 -&gt; 1.9.0</li> <li>Curator: 2.6.0 -&gt; 2.11.1</li> <li>Javassist: 3.19.0-GA -&gt; 3.28.0-GA</li> <li>Jersey: 2.25 -&gt; 2.36</li> <li>Paranamer: 2.6 -&gt; 2.8</li> <li>Reflections: 0.9.10 -&gt; 0.9.12</li> </ul>"},{"location":"deprecations/","title":"Deprecations","text":"<p>This page describes deprecated code which has been removed in Gaffer 2 and how to migrate to better equivalents. Each heading for a section below refers to a classname from <code>uk.gov.gchq.gaffer</code> where there have been changes or where that class has been removed entirely. The section headings link to the code on GitHub for that class (as of the Gaffer 1.21.1 release).</p> <p>Deprecations impacting the serialisers used in schemas are listed first, followed by changes to Seed Matching and changes to Traits. Other deprecations are then listed in alphabetical order.</p>"},{"location":"deprecations/#serialisers","title":"Serialisers","text":""},{"location":"deprecations/#migrating-away-from-deprecated-serialisers","title":"Migrating away from deprecated Serialisers","text":"<p>Various deprecated serialisers have been removed completely (details below). If any of these are being used in an existing schema, a new graph and schema will need to be created (see below for replacement serialisers to use) and data from existing graphs migrated. Data will need to be migrated (export and reimport) from graphs using deprecated serialisers before upgrading to Gaffer v2.</p> <p>It is essential to migrate data stored using deprecated serialisers. Simply replacing these serialisers is not enough because this will prevent existing data from being read and potentially put the backing store into a corrupted state.</p>"},{"location":"deprecations/#preservation-of-ordering","title":"Preservation of ordering","text":"<p>When using an ordered store (such as Accumulo), all serialisers used on vertices must preserve order. As such, <code>compactRaw</code> serialisers (which do not preserve order) cannot be used on vertices in ordered stores.</p> <p>However, when preserving order is not required, such as for properties, <code>compactRaw</code> serialisers are the most effective solution and should always be used. Using an ordered serialiser on a property would reduce performance without providing any benefit. See the schemas documentation for more detail.</p>"},{"location":"deprecations/#serialisationimplementationrawrawdateserialiser","title":"<code>serialisation.implementation.raw.RawDateSerialiser</code>","text":"<ul> <li>This class has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser</code> instead - note that this will preserve order.</li> </ul>"},{"location":"deprecations/#serialisationdateserialiser","title":"<code>serialisation.DateSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser</code> instead - note that this will preserve order. This doesn't implement <code>.deserialiseString(String)</code>, instead use <code>new Date(Long.parseLong(value))</code> in place of this.</li> </ul>"},{"location":"deprecations/#serialisationimplementationrawrawdoubleserialiser","title":"<code>serialisation.implementation.raw.RawDoubleSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser</code> instead - note that this will preserve order.</li> </ul>"},{"location":"deprecations/#serialisationdoubleserialiser","title":"<code>serialisation.DoubleSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser</code> instead - note that this will preserve order. This doesn't implement <code>.deserialiseString(String)</code>, instead use <code>Double.parseDouble(value)</code> in place of this.</li> </ul>"},{"location":"deprecations/#serialisationimplementationrawrawfloatserialiser","title":"<code>serialisation.implementation.raw.RawFloatSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser</code> instead - note that this will preserve order.</li> </ul>"},{"location":"deprecations/#serialisationfloatserialiser","title":"<code>serialisation.FloatSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser</code> instead - note that this will preserve order.</li> </ul>"},{"location":"deprecations/#serialisationintegerserialiser","title":"<code>serialisation.IntegerSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser</code> instead, this will preserve order.</li> <li>If object ordering does not need to be preserved, <code>uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser</code> could also be used instead.</li> <li>Neither of these implement <code>.deserialiseString(String)</code>, instead use <code>Integer.parseInt(value)</code> in place of this.</li> </ul>"},{"location":"deprecations/#serialisationimplementationrawrawintegerserialiser","title":"<code>serialisation.implementation.raw.RawIntegerSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser</code> instead, this will preserve order.</li> <li>If object ordering does not need to be preserved, <code>uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser</code> should instead be used.</li> </ul>"},{"location":"deprecations/#serialisationlongserialiser","title":"<code>serialisation.LongSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser</code> instead, this will preserve order.</li> <li>If object ordering does not need to be preserved, <code>uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser</code> could also be used instead.</li> </ul>"},{"location":"deprecations/#serialisationimplementationrawrawlongserialiser","title":"<code>serialisation.implementation.raw.RawLongSerialiser</code>","text":"<ul> <li>This has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser</code> instead, this will preserve order.</li> <li>If object ordering does not need to be preserved, <code>uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser</code> should instead be used.</li> </ul>"},{"location":"deprecations/#serialisationtobytesserialiser","title":"<code>serialisation.ToBytesSerialiser</code>","text":"<ul> <li>The method <code>deserialise(byte[])</code> has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface.</li> <li>It is recommended for speed/performance to use the other implementation with an offset and a length - <code>deserialise(byte[], int, int)</code>.</li> </ul>"},{"location":"deprecations/#serialisationtobytesviastringdeserialiser","title":"<code>serialisation.ToBytesViaStringDeserialiser</code>","text":"<ul> <li>The method <code>deserialise(byte[])</code> has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface.</li> <li>It is recommended for speed/performance to use the other implementation with an offset and a length - <code>deserialise(byte[], int, int)</code>.</li> </ul>"},{"location":"deprecations/#removal-of-seed-matching","title":"Removal of Seed Matching","text":""},{"location":"deprecations/#operationseedmatching","title":"<code>operation.SeedMatching</code>","text":"<ul> <li>This class has been removed.</li> <li>Use a View instead. See the Gaffer v1 docs for more detail on how to replace seed matching with a view.</li> </ul>"},{"location":"deprecations/#changes-to-store-traits","title":"Changes to Store Traits","text":""},{"location":"deprecations/#storestore","title":"<code>store.Store</code>","text":"<ul> <li>The method <code>getTraits()</code> has been removed. Use <code>Store.execute(Operation, Context)</code> with the <code>GetTraits</code> operation instead.</li> <li>The method <code>hasTrait(StoreTrait)</code> has been removed. Use <code>Store.execute(Operation, Context)</code> with the <code>HasTrait</code> operation instead.</li> </ul>"},{"location":"deprecations/#federatedstorefederatedgraphstorage","title":"<code>federatedstore.FederatedGraphStorage</code>","text":"<ul> <li>The method <code>getTraits(GetTraits, Context)</code> has been removed. Use <code>Store.execute(Operation, Context)</code> with the <code>GetTraits</code> operation instead.</li> </ul>"},{"location":"deprecations/#federatedstorefederatedstore","title":"<code>federatedstore.FederatedStore</code>","text":"<ul> <li>The methods <code>getTraits()</code> and <code>getTraits(GetTraits, Context)</code> have been removed. Use <code>Store.execute(Operation, Context)</code> with the <code>GetTraits</code> operation instead.</li> </ul>"},{"location":"deprecations/#all-other-deprecations","title":"All other Deprecations","text":""},{"location":"deprecations/#accumulostoreaccumuloproperties","title":"<code>accumulostore.AccumuloProperties</code>","text":"<ul> <li>The <code>TABLE</code> setting/variable plus the methods <code>getTable()</code> and <code>setTable(String)</code> have been removed. For <code>getTable()</code>, uk.gov.gchq.gaffer.accumulostore.getTableName() could be used instead.</li> <li>A <code>graphId</code> should be supplied instead of setting <code>TABLE</code> directly.</li> </ul>"},{"location":"deprecations/#accumulostoremockaccumulostore","title":"<code>accumulostore.MockAccumuloStore</code>","text":"<ul> <li>This class has been removed.</li> <li>For in memory graphs, use <code>uk.gov.gchq.gaffer.mapstore.MapStore</code> instead.</li> <li>For tests use <code>uk.gov.gchq.gaffer.accumulostore.MiniAccumuloStore</code> instead.</li> </ul>"},{"location":"deprecations/#commonutiltesttypes","title":"<code>commonutil.TestTypes</code>","text":"<ul> <li>This class has been removed.</li> <li>Use the equivalent <code>TestTypes</code> class in the store module <code>uk.gov.gchq.gaffer.store.TestTypes</code> instead.</li> </ul>"},{"location":"deprecations/#dataelementdefinitionviewnamedviewdetail","title":"<code>data.elementdefinition.view.NamedViewDetail</code>","text":"<ul> <li>The method <code>hasWriteAccess(final String userId, final Set&lt;String&gt; opAuths, final String adminAuth)</code> has been removed.</li> <li>Use <code>hasWriteAccess(final User user, final String adminAuth)</code> instead.</li> </ul>"},{"location":"deprecations/#dataelementdefinitionviewviewelementdefinition","title":"<code>data.elementdefinition.view.ViewElementDefinition</code>","text":"<ul> <li>The method <code>setAggregator(final ElementAggregator aggregator)</code> has been removed.</li> <li>A <code>ViewElementDefinition</code> should be constructed using the builder <code>uk.gov.gchq.gaffer.data.elementdefinition.view.ViewElementDefinition.Builder</code> instead.</li> </ul>"},{"location":"deprecations/#federatedstorefederatedaccess","title":"<code>federatedstore.FederatedAccess</code>","text":"<ul> <li>The method <code>isAddingUser(User)</code> has been removed.</li> <li>Use <code>hasReadAccess(User user, String adminAuth)</code>/<code>hasWriteAccess(User user, String adminAuth)</code> instead.</li> </ul>"},{"location":"deprecations/#federatedstorefederatedgraphstorage_1","title":"<code>federatedstore.FederatedGraphStorage</code>","text":"<ul> <li>The methods <code>getAllIdsAsAdmin()</code>, <code>getAllGraphAndAccessAsAdmin(List&lt;String&gt;)</code> and <code>changeGraphAccessAsAdmin(String, FederatedAccess)</code> have all been removed.</li> <li>The method <code>remove(String graphId)</code> has been removed. The following can be used instead:<ul> <li><code>remove(String graphId, User user)</code></li> <li><code>remove(String graphId, User user, String adminAuth)</code></li> <li><code>remove(String graphId, Predicate&lt;Entry&lt;FederatedAccess, Set&lt;Graph&gt;&gt;&gt; entryPredicateForGraphRemoval)</code></li> </ul> </li> </ul>"},{"location":"deprecations/#federatedstorefederatedstore_1","title":"<code>federatedstore.FederatedStore</code>","text":"<ul> <li>The method <code>updateOperationForGraph(Operation, Graph)</code> has been removed. Use <code>FederatedStoreUtil.updateOperationForGraph(Operation, Graph)</code> instead.</li> <li>The method <code>addGraphs(Set&lt;String&gt; graphAuths, String addingUserId, GraphSerialisable... graphs)</code> has been removed. The following can be used instead:<ul> <li><code>addGraphs(Set&lt;String&gt; graphAuths, String addingUserId, boolean isPublic, GraphSerialisable... graphs)</code></li> <li><code>addGraphs(Set&lt;String&gt; graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, GraphSerialisable... graphs)</code></li> <li><code>addGraphs(Set&lt;String&gt; graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, AccessPredicate readAccessPredicate, AccessPredicate writeAccessPredicate, GraphSerialisable... graphs)</code></li> <li><code>addGraphs(FederatedAccess access, GraphSerialisable... graphs)</code></li> </ul> </li> </ul>"},{"location":"deprecations/#federatedstoreoperationremovegraph","title":"<code>federatedstore.operation.RemoveGraph</code>","text":"<ul> <li>The method <code>Builder.setGraphId(final String graphId)</code> has been removed.</li> <li>Use <code>Builder.graphId(final String graphId)</code> which has identical behaviour instead.</li> </ul>"},{"location":"deprecations/#graphgraph","title":"<code>graph.Graph</code>","text":"<ul> <li>The methods <code>Builder.graphId</code>, <code>Builder.library</code>, <code>Builder.view</code>, <code>Builder.addHook</code>, <code>Builder.addHooks</code> have all been removed in all forms.</li> <li>Instead of using these methods, use <code>.config()</code> to set the <code>graphConfig</code>.</li> </ul>"},{"location":"deprecations/#hdfsoperationmapreduce","title":"<code>hdfs.operation.MapReduce</code>","text":"<ul> <li>The methods <code>getNumReduceTasks()</code> and <code>setNumReduceTasks(Integer)</code> have been removed.</li> <li>Gaffer\u2019s operations that inherit <code>MapReduce</code> did not make use of <code>numReduceTasks</code>, either setting it to a constant number in the <code>JobFactory</code> or using Accumulo to automatically set the number (recommended for performance) and using min/max to keep it within a range. Therefore, <code>numReduceTasks</code>, <code>getNumReduceTasks</code> and <code>setNumReduceTasks</code> have been removed from this interface.</li> </ul>"},{"location":"deprecations/#hdfsoperationaddelementsfromhdfs","title":"<code>hdfs.operation.AddElementsFromHdfs</code>","text":"<ul> <li>The methods <code>getNumReduceTasks()</code> and <code>setNumReduceTasks(Integer)</code> have been removed.</li> <li>The number of reduce tasks should not be set.  By default the number of reduce tasks should match the number of tablets.  Use minimum and maximum reduce tasks to specify boundaries for the number of reduce tasks.</li> </ul>"},{"location":"deprecations/#hdfsoperationsampledataforsplitpoints","title":"<code>hdfs.operation.SampleDataForSplitPoints</code>","text":"<ul> <li>The methods <code>getNumReduceTasks()</code> and <code>setNumReduceTasks(Integer)</code> have been removed.</li> <li>These methods were not required as <code>NumReduceTasks</code> was always set to 1 in any case.</li> </ul>"},{"location":"deprecations/#jobtrackerjobdetail","title":"<code>jobtracker.JobDetail</code>","text":"<ul> <li>The constructors which took <code>userId</code> as a <code>String</code> have been removed.</li> <li>Instead, a <code>User</code> (<code>uk.gov.gchq.gaffer.user.User</code>) should be used in its place. See the Builder for User.</li> <li><code>getUserId</code> and <code>setUserId</code> have also been removed. For getting the <code>UserId</code>, <code>getUser().getUserId()</code> can be used instead. See the Javadoc for User.</li> </ul>"},{"location":"deprecations/#jsonserialisationjsonserialiser","title":"<code>jsonserialisation.JSONSerialiser</code>","text":"<ul> <li>The method <code>update(final String jsonSerialiserClass, final String jsonSerialiserModules)</code> has been removed.</li> <li>Use <code>update(final String jsonSerialiserClass, final String jsonSerialiserModules, final Boolean strictJson)</code> instead. Passing <code>strictJson</code> as <code>null</code> will result in the same behaviour.</li> </ul>"},{"location":"deprecations/#operationoperation","title":"<code>operation.Operation</code>","text":"<ul> <li>The method <code>asOperationChain(final Operation operation)</code> has been removed.</li> <li>Use <code>OperationChain.wrap</code> with the <code>Operation</code> instead.</li> </ul>"},{"location":"deprecations/#operationimplgetwalks","title":"<code>operation.impl.GetWalks</code>","text":"<ul> <li>The method <code>Builder.operation</code> has been removed.</li> <li>Use the vararg method <code>Builder.addOperations</code> instead.</li> </ul>"},{"location":"deprecations/#operationimplsplitstore","title":"<code>operation.impl.SplitStore</code>","text":"<ul> <li>This class has been removed.</li> <li>It is replaced by <code>SplitStoreFromFile</code> which is identical except in name.</li> </ul>"},{"location":"deprecations/#operationimpljoinmethodsjoinfunction","title":"<code>operation.impl.join.methods.JoinFunction</code>","text":"<ul> <li>The method <code>join(final Iterable keys, final String keyName, final String matchingValuesName, final Match match, final Boolean flatten)</code> which was not implemented has been removed.</li> </ul>"},{"location":"deprecations/#restsystemproperty","title":"<code>rest.SystemProperty</code>","text":"<ul> <li><code>GRAPH_ID</code>, <code>GRAPH_HOOKS_PATH</code>, <code>GRAPH_LIBRARY_PATH</code> and <code>GRAPH_LIBRARY_CONFIG</code> have been removed.</li> <li>These config options have been removed in favour of providing a <code>graphConfig</code> JSON and using <code>GRAPH_CONFIG_PATH</code> instead.</li> </ul>"},{"location":"deprecations/#restservicev2exampleexamplesfactory","title":"<code>rest.service.v2.example.ExamplesFactory</code>","text":"<ul> <li>This class has been removed.</li> <li>It is replaced by <code>uk.gov.gchq.gaffer.rest.factory.ExamplesFactory</code>, which can be used instead.</li> </ul>"},{"location":"deprecations/#storestoreproperties","title":"<code>store.StoreProperties</code>","text":"<ul> <li>Store ID (<code>gaffer.store.id</code>) and related methods (<code>getId()</code> + <code>setId(String)</code>) have been removed.</li> <li>The ID is instead set in <code>GraphLibrary</code> when adding (with <code>add</code>) the <code>StoreProperties</code>.</li> <li>See the Javadoc for GraphLibrary for more detail.</li> </ul>"},{"location":"deprecations/#storecontext","title":"<code>store.Context</code>","text":"<ul> <li>The private constructor <code>Context(final User user, final Map&lt;String, Object&gt; config, final String jobId)</code> has been removed; along with the <code>jobId(String)</code> method.</li> <li>Use <code>Context(final User user, final Map&lt;String, Object&gt; config)</code> instead. This does not support supplying the Job ID, this will be set automatically. To get the Job ID use <code>.getJobId()</code>.</li> </ul>"},{"location":"deprecations/#storeschematypedefinition","title":"<code>store.schema.TypeDefinition</code>","text":"<ul> <li>The method <code>getSerialiserClass()</code> has been removed. Instead, use <code>getSerialiser()</code> with <code>.getClass()</code> and related methods.</li> <li>The method <code>setSerialiserClass(String)</code> has been removed. Instead, set the Serialiser directly using <code>setSerialiser(Serialiser)</code>.</li> </ul>"},{"location":"deprecations/#storeschemaschema","title":"<code>store.schema.Schema</code>","text":"<ul> <li>Schema ID (<code>gaffer.store.id</code>) and related methods have been removed. The ID is now defined in <code>GraphLibrary</code> when adding the schema.</li> <li><code>timestampProperty</code> and related methods have been removed. Instead, this is specified by setting <code>\"config\": {\"timestampProperty\": \"timestamp\"}</code> (where <code>\"timestamp\"</code> is the property name to use as a time stamp) in the Schema. See this example schema for more info.</li> <li>The method <code>getVertexSerialiserClass()</code> has been removed. It can be replaced by calling <code>vertexSerialiser.getClass()</code> and converting the result as appropriate, e.g. <code>getVertexSerialiserClass()</code> used <code>SimpleClassNameIdResolver.getSimpleClassName(vertexSerialiser.getClass())</code>.</li> </ul>"},{"location":"deprecations/#storelibrarygraphlibrary","title":"<code>store.library.GraphLibrary</code>","text":"<ul> <li>The method <code>addSchema(final Schema schema)</code> has been removed. Use <code>addSchema(final String id, final Schema schema)</code> instead.</li> <li>The method <code>addProperties(final StoreProperties properties)</code> has been removed. Use <code>addProperties(final String id, final StoreProperties properties)</code> instead.</li> <li>Both of these now require the schema ID to be supplied.</li> </ul>"},{"location":"deprecations/#storeoperationoperationchainvalidator","title":"<code>store.operation.OperationChainValidator</code>","text":"<ul> <li>The method <code>validateViews(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store)</code> has been removed. Use <code>validateViews(final Operation op, final User user, final Store store, final ValidationResult validationResult)</code> instead, passing <code>user</code> as <code>null</code> will result in the same behaviour.</li> <li>The method <code>validateComparables(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store)</code> has been removed. Use <code>validateComparables(final Operation op, final User user, final Store store, final ValidationResult validationResult)</code> instead, passing <code>user</code> as <code>null</code> will result in the same behaviour.</li> </ul>"},{"location":"deprecations/#storeoperationhandlernamedcachenamedviewcache","title":"<code>store.operation.handler.named.cache.NamedViewCache</code>","text":"<ul> <li>The method <code>deleteNamedView(final String name)</code> has been removed. Use <code>deleteNamedView(final String name, final User user)</code> instead, passing <code>user</code> as <code>null</code> will result in the same behaviour.</li> <li>The method <code>getNamedView(final String name)</code> has been removed. Use <code>getNamedView(final String name, final User user)</code> instead.</li> <li>The method <code>getAllNamedViews()</code> has been removed. Use <code>getAllNamedViews(final User user)</code> instead.</li> </ul>"},{"location":"deprecations/#typesintegerfreqmap","title":"<code>types.IntegerFreqMap</code>","text":"<ul> <li>This class has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.types.FreqMap</code> instead, this is identical except for using Long rather than Integer.</li> </ul>"},{"location":"deprecations/#typesfunctionintegerfreqmapaggregator","title":"<code>types.function.IntegerFreqMapAggregator</code>","text":"<ul> <li>This class has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.types.function.FreqMapAggregator</code> instead.</li> </ul>"},{"location":"deprecations/#serialisationintegerfreqmapserialiser","title":"<code>serialisation.IntegerFreqMapSerialiser</code>","text":"<ul> <li>This class has been removed.</li> <li>Use <code>uk.gov.gchq.gaffer.serialisation.FreqMapSerialiser</code> instead.</li> </ul>"},{"location":"federation-changes/","title":"Federated Store Changes","text":"<p>This page contains information on the changes to Gaffer's Federated Store. This functionality was introduced in version <code>2.0.0-alpha-0.4</code> of Gaffer. The main changes were the addition of the Federated Operation, and a change to how results are merged by default.</p>"},{"location":"federation-changes/#the-federated-operation","title":"The Federated Operation","text":"<p>The <code>FederatedOperationChain</code> was removed and replaced with a new Operation, the <code>FederatedOperation</code>. This was added to improve the control you have over how operations are federated. The Federated Operation has 3 key parameters: <code>operation</code>, <code>graphIds</code> and <code>mergeFunction</code>: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.get.GetAllElements\"\n},\n\"graphIds\": [ \"graphA\", \"graphB\" ],\n\"mergeFunction\": {\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.util.ConcatenateMergeFunction\"\n}\n}\n</code></pre></p>"},{"location":"federation-changes/#required-parameter-operation","title":"Required parameter: operation","text":"<p>This is the Operation you wish to be federated to the subgraphs. This can be a single Operation or an OperationChain. If you use an OperationChain, then the whole chain will be sent to the subgraphs.  </p>"},{"location":"federation-changes/#optional-parameter-graphids","title":"Optional parameter: graphIds","text":"<p>This is a list of graph IDs which you want to send the operation to.  </p> <p>If the user does not specify <code>graphIds</code> in the Operation, then the <code>storeConfiguredGraphIds</code> for that store will be used. If the admin has not configured the <code>storeConfiguredGraphIds</code> then all graphIds will be used.  </p> <p>For information on sending different operations in one chain to different subgraphs, see below.  </p>"},{"location":"federation-changes/#optional-parameter-mergefunction","title":"Optional parameter: mergeFunction","text":"<p>The <code>mergeFunction</code> parameter is the Function you want to use when merging the results from the subgraphs.  </p> <p>If the user does not specify a <code>mergeFunction</code> then it will be selected from the <code>storeConfiguredMergeFunctions</code> for that store. If the admin has not configured the <code>storeConfiguredMergeFunctions</code>, it will contain pre-populated <code>mergeFunctions</code>. Lastly, if a suitable <code>mergeFunction</code> is not found then a default <code>ConcatenateMergeFunction</code> is used.  </p> <p>For example, when GetElements is used as the operation inside a FederatedOperation and the user hasn't specified a <code>mergeFunction</code>, the pre-populated <code>ApplyViewToElementsFunction</code> will be selected from <code>storeConfiguredMergeFunctions</code>, unless the admin configured it to use something else.  </p>"},{"location":"federation-changes/#migrating-to-a-federatedoperation","title":"Migrating to a FederatedOperation","text":"<p>Previously, graphIds were selected with the now deprecated option: <code>gaffer.federatedstore.operation.graphIds</code>. This is being partially supported temporarily while users migrate to using a FederatedOperation, but there are some scenarios in which the option will no longer work.</p>"},{"location":"federation-changes/#sending-an-operation-to-specific-stores","title":"Sending an Operation to specific stores","text":"<p>As mentioned, the <code>gaffer.federatedstore.operation.graphIds</code> option is still being temporarily supported so if you have an Operation using that option, it should continue to work. It will still work if the option is being used inside an OperationChain. However, if the option is being used on an OperationChain, then see below. Despite the option being still supported, we still recommend you migrate to using a FederatedOperation.  </p>"},{"location":"federation-changes/#deprecated-graphids-option-on-a-single-operation","title":"Deprecated graphIds option on a single Operation","text":"<pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.get.GetAllElements\",\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphA\"\n}\n}\n</code></pre>"},{"location":"federation-changes/#new-federatedoperation-graphids-on-a-single-operation","title":"New FederatedOperation graphIds on a single Operation","text":"<pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.get.GetAllElements\"\n},\n\"graphIds\": [ \"graphA\" ]\n}\n</code></pre>"},{"location":"federation-changes/#deprecated-graphids-option-inside-an-operationchain","title":"Deprecated graphIds option inside an OperationChain","text":"<pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"ExampleOperation1\",\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphA\"\n}\n},\n{\n\"class\": \"ExampleOperation2\",\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphB\"\n}\n}\n]\n}\n</code></pre>"},{"location":"federation-changes/#new-federatedoperation-graphids-inside-an-operationchain","title":"New FederatedOperation graphIds inside an OperationChain","text":"<pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation1\"\n},\n\"graphIds\": [ \"graphA\" ]\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation2\"\n},\n\"graphIds\": [ \"graphB\" ]\n}\n]\n}\n</code></pre>"},{"location":"federation-changes/#breaking-change","title":"Breaking change","text":""},{"location":"federation-changes/#no-longer-supported-graphids-option-on-an-operationchain","title":"No longer supported graphIds option on an OperationChain","text":"<p>Previously, if you wanted to send an entire OperationChain to a specific subgraph, you could use the <code>graphIds</code> option on the chain, like so: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.get.GetAllElements\"\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.Count\"\n}\n],\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphA\"\n}\n}\n</code></pre></p>"},{"location":"federation-changes/#new-federatedoperation-graphids-on-an-operationchain","title":"New FederatedOperation graphIds on an OperationChain","text":"<p>Using the <code>graphIds</code> option on an OperationChain, as shown above, is no longer supported. You should instead wrap the chain in a FederatedOperation: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.get.GetAllElements\"\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.operation.impl.Count\"\n}\n]\n},\n\"graphIds\": [ \"graphA\" ]\n}\n</code></pre></p>"},{"location":"federation-changes/#default-results-merging","title":"Default results merging","text":"<p>As described above, FederatedStores now have <code>storeConfiguredMergeFunctions</code> that dictate how the FederatedStore will merge results from different subgraphs dependent on the Operation.  </p> <p>In places, these new defaults do differ from previous behaviour, hence results will too. This can be overriden on a per Operation basis using the <code>mergeFunction</code> parameter described above, or a per store basis by overriding <code>storeConfiguredMergeFunctions</code>. The previous behaviour was that all Operation results were concatenated together, this is now a mergeFunction within Gaffer called <code>ConcatenateMergeFunction</code>. Therefore, if you wanted a FederatedOperation to use this old behaviour, you can set the <code>mergeFunction</code> to <code>ConcatenateMergeFunction</code> (as shown above).  </p>"},{"location":"federation-changes/#new-merge-function-examples","title":"New Merge function examples","text":"<p>By default, <code>GetElements</code> results will be merged with <code>ApplyViewToElementsFunction</code>. This uses the View from the operation and applies it to all of the results, meaning the results are now re-aggregated and re-filtered using the Schema, locally in the FederatedStore. This makes the results look like they came from one graph, rather than getting back a list of Elements from different subgraphs.  </p> <p>By default, <code>GetTraits</code> results will be merged with <code>CollectionIntersect</code>. This returns the intersection of common store traits from the subgraphs. This behaviour is the same, but now it can be overriden.  </p> <p>By default, <code>GetSchema</code> results will be merged with <code>MergeSchema</code>. This returns an aggregated schema from the subgraphs, unless there is a conflict. This behaviour is the same, but now it can be overriden. For example, you may wish to use the <code>ConcatenateMergeFunction</code> if there is a schema conflict.  </p>"},{"location":"federation-changes/#default-storeconfiguredmergefunctions","title":"Default storeConfiguredMergeFunctions","text":"Operation Merge function GetElements ApplyViewToElementsFunction GetAllElements ApplyViewToElementsFunction GetSchema MergeSchema GetTraits CollectionIntersect others ConcatenateMergeFunction"},{"location":"federation-changes/#removal-of-federatedoperationchain","title":"Removal of FederatedOperationChain","text":"<p>The FederatedOperationChain has been removed, and where you would have used it before you should instead use a FederatedOperation with an OperationChain inside.  </p> <p>This is useful if you have an OperationChain and want to send different parts of the chain to different subgraphs.</p>"},{"location":"federation-changes/#individually-sending-a-sequence-of-operations-to-a-subgraph","title":"Individually sending a sequence of Operations to a subgraph","text":"<p>You could send a sequence of operations within one chain to the same subgraph using <code>graphIds</code>, however, this is not always efficient: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation1\"\n},\n\"graphIds\": [ \"graphA\" ]\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation2\"\n},\n\"graphIds\": [ \"graphA\" ]\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation3\"\n},\n\"graphIds\": [ \"graphB\" ]\n}\n]\n}\n</code></pre></p>"},{"location":"federation-changes/#removed-federatedoperationchain-sending-a-sequence-of-operations-to-a-subgraph","title":"Removed FederatedOperationChain sending a sequence of operations to a subgraph","text":"<p>It is more efficient to group together sequences of Operations that will go to the same subgraph. This used to be done with a FederatedOperationChain: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperationChain\",\n\"operations\": {\n[\n\"class\": \"ExampleOperation1\",\n\"class\": \"ExampleOperation2\"\n]\n},\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphA\"\n}\n},\n{\n\"class\": \"ExampleOperation3\",\n\"options\": {\n\"gaffer.federatedstore.operation.graphIds\": \"graphB\"\n}\n}\n]\n}\n</code></pre></p>"},{"location":"federation-changes/#new-federatedoperation-sending-a-sequence-of-operations-to-a-subgraph","title":"New FederatedOperation sending a sequence of operations to a subgraph","text":"<p>Now you should instead wrap an OperationChain inside a FederatedOperation: <pre><code>{\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": [\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"uk.gov.gchq.gaffer.operation.OperationChain\",\n\"operations\": {\n[\n\"class\": \"ExampleOperation1\",\n\"class\": \"ExampleOperation2\"\n]\n}\n},\n\"graphIds\": [ \"graphA\" ]\n},\n{\n\"class\": \"uk.gov.gchq.gaffer.federatedstore.operation.FederatedOperation\",\n\"operation\": {\n\"class\": \"ExampleOperation3\"\n},\n\"graphIds\": [ \"graphB\" ]\n}\n]\n}\n</code></pre></p>"},{"location":"log4j/","title":"Log4j in Gaffer","text":"<p>This page contains information on how logging is done in Gaffer and on previous use of Log4j in Gaffer.</p>"},{"location":"log4j/#log4j-version","title":"Log4j Version","text":"<p>Log4j version 1 (1.2.17), was used by Gaffer versions 1.21 and below. From Gaffer 1.22, Log4j was replaced with Reload4j. The newer version of Log4j, Log4j2 - which is susceptible to the major Log4Shell attack, has never been used by Gaffer or its dependencies. </p>"},{"location":"log4j/#how-logging-is-done","title":"How Logging is done","text":"<p>Gaffer uses SLF4J (Simple Logging Facade for Java) for all logging. This is a framework/abstraction layer which allows for different loggers to be used (known as bindings). The binding used by Gaffer is <code>org.slf4j:slf4j-reload4j:jar:1.7.36</code>.</p>"},{"location":"log4j/#impact-of-log4j-removal-on-projects-incorporating-gaffer","title":"Impact of Log4j removal on projects incorporating Gaffer","text":"<p>Gaffer now uses Reload4j via SLF4J. This may impact projects which are using Gaffer if they are using Log4j directly or through a transitive dependency. To help avoid dependency conflicts, we have configured <code>maven-enforcer-plugin</code> to block use of Log4j with Gaffer. If you are using Gaffer in your project and your build fails because of this plugin, you will need to add a dependency exclusion to any dependencies which depend transitively on Log4j. These can be found by using the Maven dependency tree (ideally in verbose mode).</p>"},{"location":"log4j/#dependencies-of-gaffer-using-log4j-1217","title":"Dependencies of Gaffer using Log4j 1.2.17","text":"<p>Some major Gaffer dependencies (listed below) use Log4j internally (either directly or through SLF4J). From Gaffer version 1.22 these transitive dependencies are excluded and replaced with Reload4j, such that Log4j does not appear on the classpath at all.</p> <ul> <li>GCHQ Koryphe 1.14.0 - Uses SLF4J with Log4j.</li> <li>Apache HBase 1.3.0 - Multiple artefacts used from the group <code>org.apache.hbase</code>. All depend directly on Log4j.</li> <li>Apache Hadoop 2.6.5 - Multiple artefacts used from the group <code>org.apache.hadoop</code>. All depend directly on Log4j.</li> <li>Apache Accumulo 1.9.3 - Multiple artefacts used from the group <code>org.apache.accumulo</code>. All depend directly on Log4j.</li> <li>Apache Kafka 0.10.0.0 - Artefact depends indirectly on Log4j through a sub dependency (<code>com.101tec:zkclient</code>).</li> <li>Apache Spark 2.3.2 - Artefact depends directly on Log4j.</li> </ul>"},{"location":"log4j/#log4j-vulnerabilities","title":"Log4j Vulnerabilities","text":"<p>Current vulnerabilities in Log4j 1.12.17 relate to the JDBC, SMTP and JMS appenders, the JMS Sink and the Socket Server. Gaffer never used any of this. In its default configuration, we don't believe Gaffer is vulnerable to any of these problems. If the Log4j configuration is altered, changes could be made which may cause Gaffer to be vulnerable to one or more of the above vulnerabilities. Standard security processes to prevent unauthorised access and modification of configuration files should preclude this possibility.</p>"},{"location":"openCypher/","title":"openCypher Data Format","text":"<p>openCypher was chosen as it represents \u201cthe most widely adopted, fully-specified, and open query language for property graph databases\u201d. Allowing customers to import data from enterprise applications such as neo4j and Amazon Neptune with greater ease. One caveat to this is that there are slight differences in the System column header formats used by each, these differences are shown below. For more info regarding  the data format see here</p>"},{"location":"openCypher/#entity-node-system-column-headers","title":"Entity (Node) System column headers","text":"<ul> <li>Neptune / neo4j</li> <li>:ID     / _id  \u2013  (Required) An ID for the node -&gt; VERTEX (Gaffer)</li> <li>:LABEL  / _labels  \u2013  A label for the node -&gt; GROUP (Gaffer)</li> </ul>"},{"location":"openCypher/#edge-relationship-system-column-headers","title":"Edge (relationship) System column headers","text":"<ul> <li>Neptune   / neo4j</li> <li>:ID       / _id   \u2013   An ID for the relationship. There doesn't exist a direct mapping between \"ID\" in openCypher and                    a dedicated attribute for an Edge with Gaffer, instead this is added as a property belonging to the Edge. </li> <li>:START_ID / _start  \u2013   (Required) The node ID of the node this relationship starts from -&gt; SOURCE (Gaffer)</li> <li>:END_ID   / _end   \u2013   (Required) The node ID of the node this relationship ends at -&gt; DESTINATION (Gaffer)</li> <li>:TYPE     / _type   \u2013   A type for the relationship -&gt; GROUP (Gaffer)</li> </ul>"},{"location":"openCypher/#property-column-headers","title":"Property column headers","text":"<p>Gaffer replaces \"-\" with \"_\" from column headers whilst other non valid characters as outlined within the class  PropertiesUtil are stripped.  Both Entities and Edges can have associated properties. It's possible to specify the type of each property provided by using the proceeding format propertyname:type, the default is to treat each property as a string if a type isn't supplied. </p>"},{"location":"openCypher/#supported-data-types-and-how-gaffer-handles-each","title":"Supported Data Types and How Gaffer Handles Each","text":""},{"location":"openCypher/#serialised-to-boolean","title":"Serialised to Boolean","text":"<ul> <li>Bool or Boolean   \u2013   A Boolean field. Allowed values are true and false. Any value other than true is treated as false.</li> </ul>"},{"location":"openCypher/#serialised-to-integer","title":"Serialised to Integer","text":"<ul> <li>Byte   \u2013   A whole number in the range -128 through 127. Converted to an integer value.</li> <li>Short   \u2013   A whole number in the range -32,768 through 32,767. Converted to an integer value.</li> <li>Int   \u2013   A whole number in the range -2^31 through 2^31 - 1.</li> </ul>"},{"location":"openCypher/#serialised-to-long","title":"Serialised to Long","text":"<ul> <li>Long   \u2013   A whole number in the range -2^63 through 2^63 - 1.</li> </ul>"},{"location":"openCypher/#serialised-to-float","title":"Serialised to Float","text":"<ul> <li>Float   \u2013   A 32-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not.      Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level).</li> </ul>"},{"location":"openCypher/#serialised-to-double","title":"Serialised to Double","text":"<ul> <li>Double   \u2013   A 64-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not.     Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level).</li> </ul>"},{"location":"openCypher/#serialised-to-timestamp","title":"Serialised to TimeStamp","text":"<ul> <li>DateTime   \u2013   A Java date in one of the following ISO-8601 formats:<ul> <li>yyyy-MM-dd</li> <li>yyyy-MM-ddTHH:mm</li> <li>yyyy-MM-ddTHH:mm:ss</li> <li>yyyy-MM-ddTHH:mm:ssZ</li> </ul> </li> </ul>"},{"location":"openCypher/#serialised-to-string","title":"Serialised to String","text":"<ul> <li>String   \u2013   Quotation marks are optional. Comma, newline, and carriage return characters are automatically escaped if they are included in a string that is surrounded by double quotation marks (\").</li> <li>Char   \u2013   A Char field. Stored as a string.</li> <li>Date, LocalDate, and LocalDateTime,   \u2013   See Neo4j Temporal Instants for a description of the date, localdate, and localdatetime types.   The values are loaded verbatim as strings, without validation.</li> <li>Duration   \u2013   See the Neo4j Duration format. The values are loaded verbatim as strings, without validation.</li> <li>Point   \u2013   A point field, for storing spatial data. See Neo4j Point format. The values are loaded verbatim as strings, without validation.</li> </ul>"},{"location":"openCypher/#example-of-the-opencypher-load-format","title":"Example of the openCypher load format","text":"<p><code>:ID,  name:String,  age:Int,  lang:String,  :LABEL,  :START_ID,  :END_ID,   :TYPE,  weight:Double  v1,      \"marko\",       29,             ,  person,           ,         ,        ,  v2,        \"lop\",         ,       \"java\",software,           ,         ,        ,  e1,             ,         ,             ,        ,         v1,       v2, created,            0.4</code></p>"},{"location":"ways-of-working/","title":"Ways of Working","text":""},{"location":"ways-of-working/#git-branching-model","title":"Git branching model","text":"<p>We have adopted the GitFlow Branching Model in order to support both Gaffer v1 and v2: </p>"},{"location":"ways-of-working/#issues","title":"Issues","text":"<p>Where possible a pull request should correlate to a single GitHub issue. An issue should relate to a single functional or non-functional change - changes to alter/improve other pieces of functionality should be addressed in a separate issue in order to keep reviews atomic. The reasoning behind code changes should be documented in the GitHub issue. All resolved issues should be included in the next GitHub milestone, this enables releases to be linked to the included issues. If a code change requires users of Gaffer to make changes in order for them to adopt it then the issue should be labelled 'migration-required' and a comment should be added similar to:</p> <pre><code>### Migration Steps\n\n[Description of what needs to be done to adopt the code change with examples]\n</code></pre>"},{"location":"ways-of-working/#workflow","title":"Workflow","text":"<ul> <li>Assign yourself to the issue</li> <li>Create a new branch off develop using pattern: <code>gh-[issue number]-[issue-title]</code></li> <li>Commit your changes using descriptive commit titles</li> <li>Check and push your changes</li> <li>Create a pull request (PR) to merge your branch into develop, prefixing the PR title with \"Gh-[issue number]: \"</li> <li>If you named the branch and PR correctly, the PR should have \"Resolve #[issue-number]\" automatically added to the description after it is made. If it doesn't, then please add the issue it will resolve as a \"Linked issue\"</li> <li>If there is a significant change, please follow the same process to document the change in gaffer-doc</li> <li>The pull request will be reviewed and following any changes and approval your branch will be squashed and merged into develop</li> <li>Delete the branch</li> <li>The issue will be closed automatically</li> </ul>"},{"location":"ways-of-working/#pull-requests","title":"Pull Requests","text":"<p>Pull requests will undergo a review by a Gaffer committer to check the code changes are compliant with our coding style. This is a community so please be respectful of other members - offer encouragement, support and suggestions. </p> <p>As described in our git branching model - please raise pull requests to merge your changes in our develop branch.</p> <p>When pull requests are accepted, the reviewer should squash and merge them. This is because it keeps the develop branch clean and populated with only merge commits, rather than intermediate ones. As well as this, it makes everyone's job reviewing pull requests easier as any insecure and unreviewed intermediate commits are not included into the develop branch.</p> <p>Please agree to the GCHQ OSS Contributor License Agreement before submitting a pull request. Signing the CLA is enforced by the cla-assistant.</p>"},{"location":"ways-of-working/#documentation","title":"Documentation","text":"<p>As mentioned before, any significant changes in a PR should be accompanied with an addition to Gaffer's documentation: gaffer-doc. Smaller changes should be self documented in the tests. With this approach, any large feature or change has user friendly documentation, whereas technical or implementation details are documented for developers by the tests.</p>"},{"location":"ways-of-working/#coding-style","title":"Coding style","text":""},{"location":"ways-of-working/#java","title":"Java","text":"<p>Please ensure your coding style is consistent with the rest of the Gaffer project and the Google Java Style Guide. Your changes should pass the checkstyle and spotless plugins that are part of the continuous integration pipeline and check for code formatting and licenses. Before you push your changes you can check the checkstyle plugin passes with <code>mvn checkstyle:check</code> and check the spotless plugin passes with <code>mvn spotless:check</code>.</p>"},{"location":"ways-of-working/#python","title":"Python","text":"<p>Please ensure your coding style is consistent with the rest of the Gaffer project and the PEP 8 Style Guide. However, there are a few exceptions to the standards set by PEP8: * Module level imports at the top of the file - this will not be enforced but is recommended where it does not cause issues with the code generated by Fishbowl. * Max line length of 79 characters - the max line length that will be enforced in this project has been increased to 100 characters.</p> <p>Before you create a PR for your changes you can use autopep8 to check and fix any styling issues. The following can be run which will take into account the rule exceptions mentioned above. <code>autopep8 --exit-code -r -i -a -a --max-line-length 100 --ignore E402 .</code></p>"},{"location":"ways-of-working/#javadoc","title":"Javadoc","text":"<p>Ensure your java code has sufficient javadocs explaining what the section of code does and the intended use of it. Javadocs should be used in addition to clean readable code.</p> <p>In particular: * All public classes (not required for test classes unless an explanation of the testing is required) * public methods (not required if the functionality is obvious from the method name) * public constants (not required if the constant is obvious from the name)</p>"},{"location":"ways-of-working/#tests","title":"Tests","text":"<ul> <li>All new code should be unit tested. Where this is not possible the code should be invoked and the functionality should be tested in an integration test. In a small number of cases this will not be possible - instead steps to verify the code should be thoroughly documented.</li> <li>Tests should cover edge cases and exception cases as well as normal expected behavior.</li> <li>Keep each test decoupled and don't rely on tests running in a given order - don't save state between tests.</li> <li>For a given code change, aim to improve the code coverage.</li> <li>Unit test classes should test a single class and be named [testClass]Test.</li> <li>Integration test classes should be named [functionalityUnderTest]IT.</li> <li>Tests should be readable and self documenting.</li> <li>Each test should focus on testing one small piece of functionality invoked from a single method call.</li> <li>Tests should use JUnit 5 and assertJ.</li> <li>We suggest the following pattern:</li> </ul> <pre><code>@Test\npublic void should[DoSomething|ReturnSomething] {\n// Given\n[Setup your test here]\n// When\n[Invoke the test method]\n// Then\n[assertThat the method did what was expected]\n}\n</code></pre>"},{"location":"ways-of-working/#gaffer-2","title":"Gaffer 2","text":"<p>During the Gaffer 2 development process there is a v2-alpha branch, which acts as the develop branch for changes that will only be added to Gaffer 2.</p>"}]}