{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gaffer 2 These Docs are a work in progress for Gaffer v2, the docs for Gaffer v1 can be found here . For information and migration steps for Deprecated classes which have been removed in Gaffer 2 alpha 1, please see the deprecations page . For a summary of the current Gaffer 2 alpha roadmap, including migration steps, please see the Gaffer 2 Changelist . For information on logging in Gaffer and historic use of Log4j, please see this page . Licence Gaffer is licensed under the Apache 2 licence and is covered by Crown Copyright .","title":"Home"},{"location":"#gaffer-2","text":"These Docs are a work in progress for Gaffer v2, the docs for Gaffer v1 can be found here . For information and migration steps for Deprecated classes which have been removed in Gaffer 2 alpha 1, please see the deprecations page . For a summary of the current Gaffer 2 alpha roadmap, including migration steps, please see the Gaffer 2 Changelist . For information on logging in Gaffer and historic use of Log4j, please see this page .","title":"Gaffer 2"},{"location":"#licence","text":"Gaffer is licensed under the Apache 2 licence and is covered by Crown Copyright .","title":"Licence"},{"location":"ImportCsv/","text":"ImportCsv The ImportCsv operation allows a user to import data into a running Gaffer instance by simply providing the path to a CSV file. The data within the CSV must adhere to the openCypher format . As described in the previous link there exists small differences in the headers used between Neptune and neo4j, the ImportCsv operation handles both, it also handles both entities and edges specified within the same file. Example The following example shows how to load a CSV using the openCypher CSV format into a Gaffer graph. This step requires a running Gaffer instance. openCypher load format :ID, name:String, age:Int, lang:String, :LABEL, :START_ID, :END_ID, :TYPE, weight:Double v1, \"marko\", 29, , person, , , , v2, \"lop\", , \"java\",software, , , , e1, , , , , v1, v2, created, 0.4 From the rest-api execute the following operation from POST /graph/operation/execute. As the user you must provide the file path to the CSV file containing your data. Operation to Execute { \"class\": \"uk.gov.gchq.gaffer.operation.impl.add.ImportCsv\", \"filename\": \"path/to/data.csv\" }","title":"ImportCsv"},{"location":"ImportCsv/#importcsv","text":"The ImportCsv operation allows a user to import data into a running Gaffer instance by simply providing the path to a CSV file. The data within the CSV must adhere to the openCypher format . As described in the previous link there exists small differences in the headers used between Neptune and neo4j, the ImportCsv operation handles both, it also handles both entities and edges specified within the same file.","title":"ImportCsv"},{"location":"ImportCsv/#example","text":"The following example shows how to load a CSV using the openCypher CSV format into a Gaffer graph. This step requires a running Gaffer instance.","title":"Example"},{"location":"ImportCsv/#opencypher-load-format","text":":ID, name:String, age:Int, lang:String, :LABEL, :START_ID, :END_ID, :TYPE, weight:Double v1, \"marko\", 29, , person, , , , v2, \"lop\", , \"java\",software, , , , e1, , , , , v1, v2, created, 0.4 From the rest-api execute the following operation from POST /graph/operation/execute. As the user you must provide the file path to the CSV file containing your data.","title":"openCypher load format"},{"location":"ImportCsv/#operation-to-execute","text":"{ \"class\": \"uk.gov.gchq.gaffer.operation.impl.add.ImportCsv\", \"filename\": \"path/to/data.csv\" }","title":"Operation to Execute"},{"location":"accumulo-kerberos/","text":"Accumulo Kerberos Support This page contains information on Kerberos Authentication support for Gaffer's Accumulo Store. This functionality was introduced in version 2.0.0-alpha-0.3.1 of Gaffer. Using the Accumulo Store with Kerberos Prerequisites To use Gaffer's Accumulo Store with Kerberos authentication: The Accumulo cluster to connect with must be correctly configured to use Kerberos. A principal for the system/host Gaffer will be running on must be created in the Key Distribution Center (KDC) database. The Gaffer principal should use the standard primary/instance@realm format. Using principals without an instance qualification has not been tested. A keytab for the Gaffer principal must be created and transferred to the Gaffer host. The Gaffer principle must have been added as an Accumulo user with suitable permissions granted. Kerberos client utilities should be installed on the host and krb5.conf must be correctly configured. An Accumulo client configuration should be available on the host and contain the correct options to enable Kerberos. The Gaffer store.properties should state that Kerberos is to be used, specify the principal name and the keytab path. The sections below cover some of these points in more detail. Accumulo user for Gaffer When Kerberos is used with Accumulo, any client with a principal can connect without requiring an Accumulo user to have been created previously . This works by creating an Accumulo user automatically when a new client connects. These users are not granted any permissions. Users can still be created manually via the Accumulo shell, with Gaffer's full principal (with all components) given as the username. Permissions to create and read tables can then be granted to this user. If this isn't done, Accumulo will create the user automatically when Gaffer first connects. In this case Gaffer will fail to start as the required permissions will not have been granted - they can then be granted via the shell and Gaffer restarted. Accumulo Client configuration Depending on the version of Accumulo used, an accumulo-client.properties (2.x) or client.conf (1.x) must be populated as described in the respective version of the Accumulo documentation. The only value which needs to be altered is the Kerberos server primary. This should reflect the primary part of the principals used by the Accumulo cluster. The location of this config file can be specified using the ACCUMULO_CLIENT_CONF_PATH environment variable. If this is not set, then default paths will be checked . Other than this file, Accumulo libraries and configuration files do not need to be installed on the Gaffer host. Gaffer store.properties configuration In addition to the usual Accumulo Store settings , these extra options must be specified for Kerberos: accumulo.kerberos.enable=true accumulo.kerberos.principal=gaffer/host.domain@REALM.NAME accumulo.kerberos.keytab=/gaffer/config/gaffer.keytab The accumulo.username and accumulo.password values do not need to be set and are ignored when accumulo.kerberos.enable is true. The kinit Kerberos command does not need to be used, although it might be useful for ensuring the client principal works correctly. All Kerberos ticket management, renewal and re-login is handled automatically. Specifying a different krb5.conf If the krb5.conf in the default system location is not suitable, or if it's stored in a non-standard location, then custom a custom krb5.conf location can be specified when starting Gaffer by setting the system property value java.security.krb5.conf . The simplest way to do this is by using the option flag -Djava.security.krb5.conf=/my/path/to/krb5.conf when launching the Gaffer JAR. Federation Considerations Due to the way Kerberos is implemented in Accumulo, it is not possible for Gaffer to use multiple principals at the same time. For the FederatedStore , this prevents adding graphs which are on different Accumulo clusters, if those clusters require different principals. In practice this is unlikely to be a problem, as different Accumulo clusters would only need separate client principals if they were on separate Kerberos Realms or using different KDCs. This only impacts Accumulo clusters which require Kerberos. It doesn't impact on adding graphs which are stored in clusters using basic authentication and not Kerberos. Nor does it affect adding graphs from a Kerberos cluster and also adding graphs from a non Kerberos cluster in the same FederatedStore . If this limitation is a problem, it can be worked around by running additional Gaffer instances and connecting to them using a ProxyStore in the FederatedStore , rather than connecting directly using an AccumuloStore . HDFS Considerations When using the AddElementsFromHdfs operation Gaffer acts as a HDFS client. When Kerberos is used ( Hadoop Secure Mode ), HDFS clients must have native libraries installed and configured correctly; else Hadoop will raise a Runtime Exception stating that \"Secure IO is not possible without native code extensions\". The HDFS client also requires the Hadoop configuration files core-site.xml and hdfs-site.xml to both be present and configured as below. The location of these files can be specified using the HADOOP_CONF_DIR environment variable. <!--Properties in core-site.xml--> <property> <name> hadoop.security.authentication </name> <value> kerberos </value> </property> <property> <name> hadoop.security.authorization </name> <value> true </value> </property> In particular, hdfs-site.xml requires the yarn.resourcemanager.principal property to be set to the HDFS client principal - should be the same one as in the Gaffer Store properties. If this is missing Hadoop will fail to connect and raise an IO Exception with \"Can't get Master Kerberos principal for use as renewer\". <!--Properties in hdfs-site.xml--> <property> <name> yarn.resourcemanager.principal </name> <value> primary/instance@realm </value> </property> Note that the core-site.xml and hdfs-site.xml files are only required if AddElementsFromHdfs is going to be used. For Accumulo connections the Hadoop properties (from core-site.xml ) used for enabling Kerberos are set automatically in Gaffer's connection code. Spark Accumulo Library The Spark Accumulo Library has not yet been updated to support Kerberos. This prevents Spark Operations from being used with an AccumuloStore which has Kerberos authentication enabled. It is on the backlog for support to be added in future. Troubleshooting Kerberos is not easy to configure and familiarity with Kerberos concepts is recommended. There are some useful links to introductory information in the Accumulo Kerberos docs . Improperly configured DNS will cause problems with Kerberos. Ensure all hostnames used in Principals resolved correctly, include reverse lookup. Due to how the system's hostname is used by the Hadoop Kerberos libraries, a mismatch between the configured hostname and the hostname resolved by a reverse lookup can prevent authentication from working correctly. Various environment variables can be set for debugging Kerberos, see the Hadoop docs for more information . These variables are applicable to Accumulo ( see docs ) because its Kerberos implementation uses Hadoop libraries. The Gaffer logging level (set in log4.xml ) should be increased to at least INFO when using these environment variables.","title":"Accumulo Kerberos"},{"location":"accumulo-kerberos/#accumulo-kerberos-support","text":"This page contains information on Kerberos Authentication support for Gaffer's Accumulo Store. This functionality was introduced in version 2.0.0-alpha-0.3.1 of Gaffer.","title":"Accumulo Kerberos Support"},{"location":"accumulo-kerberos/#using-the-accumulo-store-with-kerberos","text":"","title":"Using the Accumulo Store with Kerberos"},{"location":"accumulo-kerberos/#prerequisites","text":"To use Gaffer's Accumulo Store with Kerberos authentication: The Accumulo cluster to connect with must be correctly configured to use Kerberos. A principal for the system/host Gaffer will be running on must be created in the Key Distribution Center (KDC) database. The Gaffer principal should use the standard primary/instance@realm format. Using principals without an instance qualification has not been tested. A keytab for the Gaffer principal must be created and transferred to the Gaffer host. The Gaffer principle must have been added as an Accumulo user with suitable permissions granted. Kerberos client utilities should be installed on the host and krb5.conf must be correctly configured. An Accumulo client configuration should be available on the host and contain the correct options to enable Kerberos. The Gaffer store.properties should state that Kerberos is to be used, specify the principal name and the keytab path. The sections below cover some of these points in more detail.","title":"Prerequisites"},{"location":"accumulo-kerberos/#accumulo-user-for-gaffer","text":"When Kerberos is used with Accumulo, any client with a principal can connect without requiring an Accumulo user to have been created previously . This works by creating an Accumulo user automatically when a new client connects. These users are not granted any permissions. Users can still be created manually via the Accumulo shell, with Gaffer's full principal (with all components) given as the username. Permissions to create and read tables can then be granted to this user. If this isn't done, Accumulo will create the user automatically when Gaffer first connects. In this case Gaffer will fail to start as the required permissions will not have been granted - they can then be granted via the shell and Gaffer restarted.","title":"Accumulo user for Gaffer"},{"location":"accumulo-kerberos/#accumulo-client-configuration","text":"Depending on the version of Accumulo used, an accumulo-client.properties (2.x) or client.conf (1.x) must be populated as described in the respective version of the Accumulo documentation. The only value which needs to be altered is the Kerberos server primary. This should reflect the primary part of the principals used by the Accumulo cluster. The location of this config file can be specified using the ACCUMULO_CLIENT_CONF_PATH environment variable. If this is not set, then default paths will be checked . Other than this file, Accumulo libraries and configuration files do not need to be installed on the Gaffer host.","title":"Accumulo Client configuration"},{"location":"accumulo-kerberos/#gaffer-storeproperties-configuration","text":"In addition to the usual Accumulo Store settings , these extra options must be specified for Kerberos: accumulo.kerberos.enable=true accumulo.kerberos.principal=gaffer/host.domain@REALM.NAME accumulo.kerberos.keytab=/gaffer/config/gaffer.keytab The accumulo.username and accumulo.password values do not need to be set and are ignored when accumulo.kerberos.enable is true. The kinit Kerberos command does not need to be used, although it might be useful for ensuring the client principal works correctly. All Kerberos ticket management, renewal and re-login is handled automatically.","title":"Gaffer store.properties configuration"},{"location":"accumulo-kerberos/#specifying-a-different-krb5conf","text":"If the krb5.conf in the default system location is not suitable, or if it's stored in a non-standard location, then custom a custom krb5.conf location can be specified when starting Gaffer by setting the system property value java.security.krb5.conf . The simplest way to do this is by using the option flag -Djava.security.krb5.conf=/my/path/to/krb5.conf when launching the Gaffer JAR.","title":"Specifying a different krb5.conf"},{"location":"accumulo-kerberos/#federation-considerations","text":"Due to the way Kerberos is implemented in Accumulo, it is not possible for Gaffer to use multiple principals at the same time. For the FederatedStore , this prevents adding graphs which are on different Accumulo clusters, if those clusters require different principals. In practice this is unlikely to be a problem, as different Accumulo clusters would only need separate client principals if they were on separate Kerberos Realms or using different KDCs. This only impacts Accumulo clusters which require Kerberos. It doesn't impact on adding graphs which are stored in clusters using basic authentication and not Kerberos. Nor does it affect adding graphs from a Kerberos cluster and also adding graphs from a non Kerberos cluster in the same FederatedStore . If this limitation is a problem, it can be worked around by running additional Gaffer instances and connecting to them using a ProxyStore in the FederatedStore , rather than connecting directly using an AccumuloStore .","title":"Federation Considerations"},{"location":"accumulo-kerberos/#hdfs-considerations","text":"When using the AddElementsFromHdfs operation Gaffer acts as a HDFS client. When Kerberos is used ( Hadoop Secure Mode ), HDFS clients must have native libraries installed and configured correctly; else Hadoop will raise a Runtime Exception stating that \"Secure IO is not possible without native code extensions\". The HDFS client also requires the Hadoop configuration files core-site.xml and hdfs-site.xml to both be present and configured as below. The location of these files can be specified using the HADOOP_CONF_DIR environment variable. <!--Properties in core-site.xml--> <property> <name> hadoop.security.authentication </name> <value> kerberos </value> </property> <property> <name> hadoop.security.authorization </name> <value> true </value> </property> In particular, hdfs-site.xml requires the yarn.resourcemanager.principal property to be set to the HDFS client principal - should be the same one as in the Gaffer Store properties. If this is missing Hadoop will fail to connect and raise an IO Exception with \"Can't get Master Kerberos principal for use as renewer\". <!--Properties in hdfs-site.xml--> <property> <name> yarn.resourcemanager.principal </name> <value> primary/instance@realm </value> </property> Note that the core-site.xml and hdfs-site.xml files are only required if AddElementsFromHdfs is going to be used. For Accumulo connections the Hadoop properties (from core-site.xml ) used for enabling Kerberos are set automatically in Gaffer's connection code.","title":"HDFS Considerations"},{"location":"accumulo-kerberos/#spark-accumulo-library","text":"The Spark Accumulo Library has not yet been updated to support Kerberos. This prevents Spark Operations from being used with an AccumuloStore which has Kerberos authentication enabled. It is on the backlog for support to be added in future.","title":"Spark Accumulo Library"},{"location":"accumulo-kerberos/#troubleshooting","text":"Kerberos is not easy to configure and familiarity with Kerberos concepts is recommended. There are some useful links to introductory information in the Accumulo Kerberos docs . Improperly configured DNS will cause problems with Kerberos. Ensure all hostnames used in Principals resolved correctly, include reverse lookup. Due to how the system's hostname is used by the Hadoop Kerberos libraries, a mismatch between the configured hostname and the hostname resolved by a reverse lookup can prevent authentication from working correctly. Various environment variables can be set for debugging Kerberos, see the Hadoop docs for more information . These variables are applicable to Accumulo ( see docs ) because its Kerberos implementation uses Hadoop libraries. The Gaffer logging level (set in log4.xml ) should be increased to at least INFO when using these environment variables.","title":"Troubleshooting"},{"location":"accumulo-migration/","text":"Accumulo Migration This page contains information on changes to the Accumulo/Hadoop versions supported by Gaffer and how to continue using the previously supported versions. Accumulo 2 & Hadoop 3 become default versions From the 2.0.0-alpha-3 release of Gaffer, the default version of Accumulo has been upgraded to Accumulo 2.0.1 . Hadoop has also been upgraded to the latest version (currently 3.3.3). This is because Hadoop 2.x is not compatible with Accumulo 2. Retained support for Accumulo 1 & Hadoop 2 Support for certain versions of Accumulo 1 and Hadoop 2 (specifically 1.9.3 & 2.6.5) has been retained and can be enabled by using a Maven profile when building from source. This facilitates testing with these versions and creates shaded JARs (e.g. spring-rest exec, accumulo-store iterators) with the appropriate versions of supporting libraries. As described in the source docs, other versions of Accumulo 1.x and Hadoop 2.x might also work. Building Gaffer with the 'legacy' profile To build Gaffer using Accumulo 1.9.3 and Hadoop 2.6.5, the 'legacy' Maven profile needs to be used. This is enabled by supplying -Dlegacy=true as an extra argument at the command line when running Maven. For example, mvn clean install -Pcoverage -Dlegacy=true will perform a full build/test of Gaffer with this profile enabled. Java 11 cannot be used with this profile because only Hadoop 3.3.0 and higher support it. With the 'legacy' Maven profile active, the filenames of all shaded JARs produced are appended with -legacy . This is to differentiate them from the default shaded JARs which contain different libraries and different library versions. A default Gaffer Accumulo REST API JAR will not work with an Accumulo 1 cluster, and the 'legacy' version will not work with Accumulo 2. Migrating from Accumulo 1 to 2 See the Accumulo documentation for guidance on upgrading from Accumulo 1 to 2. Of particular significance is the deprecation of the dynamic reloading classpath directory functionality in Accumulo 2. This affects where and how the Gaffer iterators JAR can be installed. See the Accumulo store documentation for these installation details. Otherwise, no Accumulo specific Gaffer configuration needs to be changed and migrating from Accumulo 1 to 2 should be as simple as swapping the Gaffer dependency versions/JARs, although this has not been actively tested.","title":"Accumulo Migration"},{"location":"accumulo-migration/#accumulo-migration","text":"This page contains information on changes to the Accumulo/Hadoop versions supported by Gaffer and how to continue using the previously supported versions.","title":"Accumulo Migration"},{"location":"accumulo-migration/#accumulo-2-hadoop-3-become-default-versions","text":"From the 2.0.0-alpha-3 release of Gaffer, the default version of Accumulo has been upgraded to Accumulo 2.0.1 . Hadoop has also been upgraded to the latest version (currently 3.3.3). This is because Hadoop 2.x is not compatible with Accumulo 2.","title":"Accumulo 2 &amp; Hadoop 3 become default versions"},{"location":"accumulo-migration/#retained-support-for-accumulo-1-hadoop-2","text":"Support for certain versions of Accumulo 1 and Hadoop 2 (specifically 1.9.3 & 2.6.5) has been retained and can be enabled by using a Maven profile when building from source. This facilitates testing with these versions and creates shaded JARs (e.g. spring-rest exec, accumulo-store iterators) with the appropriate versions of supporting libraries. As described in the source docs, other versions of Accumulo 1.x and Hadoop 2.x might also work.","title":"Retained support for Accumulo 1 &amp; Hadoop 2"},{"location":"accumulo-migration/#building-gaffer-with-the-legacy-profile","text":"To build Gaffer using Accumulo 1.9.3 and Hadoop 2.6.5, the 'legacy' Maven profile needs to be used. This is enabled by supplying -Dlegacy=true as an extra argument at the command line when running Maven. For example, mvn clean install -Pcoverage -Dlegacy=true will perform a full build/test of Gaffer with this profile enabled. Java 11 cannot be used with this profile because only Hadoop 3.3.0 and higher support it. With the 'legacy' Maven profile active, the filenames of all shaded JARs produced are appended with -legacy . This is to differentiate them from the default shaded JARs which contain different libraries and different library versions. A default Gaffer Accumulo REST API JAR will not work with an Accumulo 1 cluster, and the 'legacy' version will not work with Accumulo 2.","title":"Building Gaffer with the 'legacy' profile"},{"location":"accumulo-migration/#migrating-from-accumulo-1-to-2","text":"See the Accumulo documentation for guidance on upgrading from Accumulo 1 to 2. Of particular significance is the deprecation of the dynamic reloading classpath directory functionality in Accumulo 2. This affects where and how the Gaffer iterators JAR can be installed. See the Accumulo store documentation for these installation details. Otherwise, no Accumulo specific Gaffer configuration needs to be changed and migrating from Accumulo 1 to 2 should be as simple as swapping the Gaffer dependency versions/JARs, although this has not been actively tested.","title":"Migrating from Accumulo 1 to 2"},{"location":"changelist/","text":"Gaffer 2 Changelist Below is a summary of intended changes that will be made in Gaffer version 2. Note: this represents the current roadmap which is not final (also available on GitHub ) but the features may change . Alpha 1 | released Removal of Deprecated code All of Gaffer 1's deprecated code has been removed. Please see the deprecations page for full details. Removal of HBase and Parquet stores The HBase and Parquet stores have been removed from Gaffer in version 2. We made posts for both the HBase and Parquet stores to understand the levels of usage. It was then decided to remove both stores as this would make introducing various improvements easier in the long term. HBase and Parquet remain available in Gaffer version 1. In the future, they could be reimplemented for Gaffer 2, though we do not plan to currently. Alpha 2 | released Dependency Upgrades Dependencies have been updated, where possible to the latest version, removing vulnerabilities. Please see the dependencies page for full details. Gaffer now builds with Java 8 and Java 11 There is now a maven profile that will swap dependency versions so you can build Gaffer with Java 11. The code has also been updated to build with both Java versions. Removal of CloseableIterable The CloseableIterable class has been removed so Operations like GetAllElements now return an Iterable instead, but the result still implements Closeable. Known issues The Road Traffic example REST has a bug in this release that means the example data isn't loaded in. This means that the gafferpy tests will fail as there is no data, but it still works. Alpha 3 | released Fixes & Improvements The Spring REST Swagger UI and Road Traffic example REST have been fixed and versions of Jersey and Jackson have both been updated. Accumulo 2 Support The Accumulo store now supports Accumulo 2 and Hadoop 3 by default, with support for Accumulo 1 and Hadoop 2 retained. See the Accumulo Migration page for more information about this change. Alpha 3.1 | released CSV Export A new ExportToLocalFile Operation has been added, along with OpenCypher format CSV support. Accumulo Kerberos Authentication Support The Accumulo store now supports authenticating to Accumulo and HDFS using Kerberos, in addition to username/password. For more information, see the Kerberos support page Future Alphas Federated Store improvements and fixes Multiple bug fixes and improvements to the Federated Store. Full details TBD. Named Operation Improvements Some changes and improvements to Named Operations are planned. Full details TBD.","title":"Changelist"},{"location":"changelist/#gaffer-2-changelist","text":"Below is a summary of intended changes that will be made in Gaffer version 2. Note: this represents the current roadmap which is not final (also available on GitHub ) but the features may change .","title":"Gaffer 2 Changelist"},{"location":"changelist/#alpha-1-released","text":"","title":"Alpha 1 | released"},{"location":"changelist/#removal-of-deprecated-code","text":"All of Gaffer 1's deprecated code has been removed. Please see the deprecations page for full details.","title":"Removal of Deprecated code"},{"location":"changelist/#removal-of-hbase-and-parquet-stores","text":"The HBase and Parquet stores have been removed from Gaffer in version 2. We made posts for both the HBase and Parquet stores to understand the levels of usage. It was then decided to remove both stores as this would make introducing various improvements easier in the long term. HBase and Parquet remain available in Gaffer version 1. In the future, they could be reimplemented for Gaffer 2, though we do not plan to currently.","title":"Removal of HBase and Parquet stores"},{"location":"changelist/#alpha-2-released","text":"","title":"Alpha 2 | released"},{"location":"changelist/#dependency-upgrades","text":"Dependencies have been updated, where possible to the latest version, removing vulnerabilities. Please see the dependencies page for full details.","title":"Dependency Upgrades"},{"location":"changelist/#gaffer-now-builds-with-java-8-and-java-11","text":"There is now a maven profile that will swap dependency versions so you can build Gaffer with Java 11. The code has also been updated to build with both Java versions.","title":"Gaffer now builds with Java 8 and Java 11"},{"location":"changelist/#removal-of-closeableiterable","text":"The CloseableIterable class has been removed so Operations like GetAllElements now return an Iterable instead, but the result still implements Closeable.","title":"Removal of CloseableIterable"},{"location":"changelist/#known-issues","text":"The Road Traffic example REST has a bug in this release that means the example data isn't loaded in. This means that the gafferpy tests will fail as there is no data, but it still works.","title":"Known issues"},{"location":"changelist/#alpha-3-released","text":"","title":"Alpha 3 | released"},{"location":"changelist/#fixes-improvements","text":"The Spring REST Swagger UI and Road Traffic example REST have been fixed and versions of Jersey and Jackson have both been updated.","title":"Fixes &amp; Improvements"},{"location":"changelist/#accumulo-2-support","text":"The Accumulo store now supports Accumulo 2 and Hadoop 3 by default, with support for Accumulo 1 and Hadoop 2 retained. See the Accumulo Migration page for more information about this change.","title":"Accumulo 2 Support"},{"location":"changelist/#alpha-31-released","text":"","title":"Alpha 3.1 | released"},{"location":"changelist/#csv-export","text":"A new ExportToLocalFile Operation has been added, along with OpenCypher format CSV support.","title":"CSV Export"},{"location":"changelist/#accumulo-kerberos-authentication-support","text":"The Accumulo store now supports authenticating to Accumulo and HDFS using Kerberos, in addition to username/password. For more information, see the Kerberos support page","title":"Accumulo Kerberos Authentication Support"},{"location":"changelist/#future-alphas","text":"","title":"Future Alphas"},{"location":"changelist/#federated-store-improvements-and-fixes","text":"Multiple bug fixes and improvements to the Federated Store. Full details TBD.","title":"Federated Store improvements and fixes"},{"location":"changelist/#named-operation-improvements","text":"Some changes and improvements to Named Operations are planned. Full details TBD.","title":"Named Operation Improvements"},{"location":"dependencies/","text":"Dependency Upgrades This page lists the dependencies that have been upgraded as part of Gaffer 2. Assertj: 3.20.2 -> 3.22.0 Junit5: 5.6.0 -> 5.8.2 Mockito: 3.3.3 -> 4.3.1 Slf4j: 1.7.25 -> 1.7.36 Log4j: 1.2.17 -> Reload4j: 1.2.18.3 Koryphe: 1.14.0 -> 2.1.0 Avro: 1.7.7 -> 1.8.2 Jackson: 2.6.5 -> 2.12.6 Hazelcast: 3.8 -> 5.1 Spring Boot: 1.3.2 -> 2.5.12 Spring API Swagger: 2.6.0 -> 3.0.0 Commons-codec: 1.6 -> 1.15 Commons-io: 2.7 -> 2.11.0 Commons-lang: 3.3.2 -> 3.12.0 Commons-logging: 1.1.3 -> 1.2 Commons-math: 2.1 -> 2.2 Commons-math3: 3.4.1 -> 3.6.1 Commons-csv: 1.4 -> 1.9.0 Curator: 2.6.0 -> 2.11.1 Javassist: 3.19.0-GA -> 3.28.0-GA Jersey: 2.25 -> 2.36 Paranamer: 2.6 -> 2.8 Reflections: 0.9.10 -> 0.9.12","title":"Dependencies"},{"location":"dependencies/#dependency-upgrades","text":"This page lists the dependencies that have been upgraded as part of Gaffer 2. Assertj: 3.20.2 -> 3.22.0 Junit5: 5.6.0 -> 5.8.2 Mockito: 3.3.3 -> 4.3.1 Slf4j: 1.7.25 -> 1.7.36 Log4j: 1.2.17 -> Reload4j: 1.2.18.3 Koryphe: 1.14.0 -> 2.1.0 Avro: 1.7.7 -> 1.8.2 Jackson: 2.6.5 -> 2.12.6 Hazelcast: 3.8 -> 5.1 Spring Boot: 1.3.2 -> 2.5.12 Spring API Swagger: 2.6.0 -> 3.0.0 Commons-codec: 1.6 -> 1.15 Commons-io: 2.7 -> 2.11.0 Commons-lang: 3.3.2 -> 3.12.0 Commons-logging: 1.1.3 -> 1.2 Commons-math: 2.1 -> 2.2 Commons-math3: 3.4.1 -> 3.6.1 Commons-csv: 1.4 -> 1.9.0 Curator: 2.6.0 -> 2.11.1 Javassist: 3.19.0-GA -> 3.28.0-GA Jersey: 2.25 -> 2.36 Paranamer: 2.6 -> 2.8 Reflections: 0.9.10 -> 0.9.12","title":"Dependency Upgrades"},{"location":"deprecations/","text":"Deprecations This page describes deprecated code which has been removed in Gaffer 2 and how to migrate to better equivalents. Each heading for a section below refers to a classname from uk.gov.gchq.gaffer where there have been changes or where that class has been removed entirely. The section headings link to the code on GitHub for that class (as of the Gaffer 1.21.1 release). Deprecations impacting the serialisers used in schemas are listed first, followed by changes to Seed Matching and changes to Traits . Other deprecations are then listed in alphabetical order . Serialisers Migrating away from deprecated Serialisers Various deprecated serialisers have been removed completely (details below). If any of these are being used in an existing schema, a new graph and schema will need to be created (see below for replacement serialisers to use) and data from existing graphs migrated. Data will need to be migrated (export and reimport) from graphs using deprecated serialisers before upgrading to Gaffer v2. It is essential to migrate data stored using deprecated serialisers. Simply replacing these serialisers is not enough because this will prevent existing data from being read and potentially put the backing store into a corrupted state . Preservation of ordering When using an ordered store (such as Accumulo), all serialisers used on vertices must preserve order. As such, compactRaw serialisers (which do not preserve order) cannot be used on vertices in ordered stores. However, when preserving order is not required, such as for properties, compactRaw serialisers are the most effective solution and should always be used. Using an ordered serialiser on a property would reduce performance without providing any benefit. See the schemas documentation for more detail . serialisation.implementation.raw.RawDateSerialiser This class has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser instead - note that this will preserve order. serialisation.DateSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser instead - note that this will preserve order. This doesn't implement .deserialiseString(String) , instead use new Date(Long.parseLong(value)) in place of this. serialisation.implementation.raw.RawDoubleSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser instead - note that this will preserve order. serialisation.DoubleSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser instead - note that this will preserve order. This doesn't implement .deserialiseString(String) , instead use Double.parseDouble(value) in place of this. serialisation.implementation.raw.RawFloatSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser instead - note that this will preserve order. serialisation.FloatSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser instead - note that this will preserve order. serialisation.IntegerSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser could also be used instead. Neither of these implement .deserialiseString(String) , instead use Integer.parseInt(value) in place of this. serialisation.implementation.raw.RawIntegerSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser should instead be used. serialisation.LongSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser could also be used instead. serialisation.implementation.raw.RawLongSerialiser This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser should instead be used. serialisation.ToBytesSerialiser The method deserialise(byte[]) has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface. It is recommended for speed/performance to use the other implementation with an offset and a length - deserialise(byte[], int, int) . serialisation.ToBytesViaStringDeserialiser The method deserialise(byte[]) has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface. It is recommended for speed/performance to use the other implementation with an offset and a length - deserialise(byte[], int, int) . Removal of Seed Matching operation.SeedMatching This class has been removed. Use a View instead. See the Gaffer v1 docs for more detail on how to replace seed matching with a view. Changes to Store Traits store.Store The method getTraits() has been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead. The method hasTrait(StoreTrait) has been removed. Use Store.execute(Operation, Context) with the HasTrait operation instead. federatedstore.FederatedGraphStorage The method getTraits(GetTraits, Context) has been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead. federatedstore.FederatedStore The methods getTraits() and getTraits(GetTraits, Context) have been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead. All other Deprecations accumulostore.AccumuloProperties The TABLE setting/variable plus the methods getTable() and setTable(String) have been removed. For getTable() , uk.gov.gchq.gaffer.accumulostore.getTableName() could be used instead. A graphId should be supplied instead of setting TABLE directly. accumulostore.MockAccumuloStore This class has been removed. For in memory graphs, use uk.gov.gchq.gaffer.mapstore.MapStore instead. For tests use uk.gov.gchq.gaffer.accumulostore.MiniAccumuloStore instead. commonutil.TestTypes This class has been removed. Use the equivalent TestTypes class in the store module uk.gov.gchq.gaffer.store.TestTypes instead. data.elementdefinition.view.NamedViewDetail The method hasWriteAccess(final String userId, final Set<String> opAuths, final String adminAuth) has been removed. Use hasWriteAccess(final User user, final String adminAuth) instead. data.elementdefinition.view.ViewElementDefinition The method setAggregator(final ElementAggregator aggregator) has been removed. A ViewElementDefinition should be constructed using the builder uk.gov.gchq.gaffer.data.elementdefinition.view.ViewElementDefinition.Builder instead. federatedstore.FederatedAccess The method isAddingUser(User) has been removed. Use hasReadAccess(User user, String adminAuth) / hasWriteAccess(User user, String adminAuth) instead. federatedstore.FederatedGraphStorage The methods getAllIdsAsAdmin() , getAllGraphAndAccessAsAdmin(List<String>) and changeGraphAccessAsAdmin(String, FederatedAccess) have all been removed. The method remove(String graphId) has been removed. The following can be used instead: remove(String graphId, User user) remove(String graphId, User user, String adminAuth) remove(String graphId, Predicate<Entry<FederatedAccess, Set<Graph>>> entryPredicateForGraphRemoval) federatedstore.FederatedStore The method updateOperationForGraph(Operation, Graph) has been removed. Use FederatedStoreUtil.updateOperationForGraph(Operation, Graph) instead. The method addGraphs(Set<String> graphAuths, String addingUserId, GraphSerialisable... graphs) has been removed. The following can be used instead: addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, GraphSerialisable... graphs) addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, GraphSerialisable... graphs) addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, AccessPredicate readAccessPredicate, AccessPredicate writeAccessPredicate, GraphSerialisable... graphs) addGraphs(FederatedAccess access, GraphSerialisable... graphs) federatedstore.operation.RemoveGraph The method Builder.setGraphId(final String graphId) has been removed. Use Builder.graphId(final String graphId) which has identical behaviour instead. graph.Graph The methods Builder.graphId , Builder.library , Builder.view , Builder.addHook , Builder.addHooks have all been removed in all forms. Instead of using these methods, use .config() to set the graphConfig . hdfs.operation.MapReduce The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. Gaffer\u2019s operations that inherit MapReduce did not make use of numReduceTasks , either setting it to a constant number in the JobFactory or using Accumulo to automatically set the number (recommended for performance) and using min/max to keep it within a range. Therefore, numReduceTasks , getNumReduceTasks and setNumReduceTasks have been removed from this interface. hdfs.operation.AddElementsFromHdfs The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. The number of reduce tasks should not be set. By default the number of reduce tasks should match the number of tablets. Use minimum and maximum reduce tasks to specify boundaries for the number of reduce tasks. hdfs.operation.SampleDataForSplitPoints The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. These methods were not required as NumReduceTasks was always set to 1 in any case. jobtracker.JobDetail The constructors which took userId as a String have been removed. Instead, a User ( uk.gov.gchq.gaffer.user.User ) should be used in its place. See the Builder for User . getUserId and setUserId have also been removed. For getting the UserId , getUser().getUserId() can be used instead. See the Javadoc for User . jsonserialisation.JSONSerialiser The method update(final String jsonSerialiserClass, final String jsonSerialiserModules) has been removed. Use update(final String jsonSerialiserClass, final String jsonSerialiserModules, final Boolean strictJson) instead. Passing strictJson as null will result in the same behaviour. operation.Operation The method asOperationChain(final Operation operation) has been removed. Use OperationChain.wrap with the Operation instead. operation.impl.GetWalks The method Builder.operation has been removed. Use the vararg method Builder.addOperations instead. operation.impl.SplitStore This class has been removed. It is replaced by SplitStoreFromFile which is identical except in name. operation.impl.join.methods.JoinFunction The method join(final Iterable keys, final String keyName, final String matchingValuesName, final Match match, final Boolean flatten) which was not implemented has been removed. rest.SystemProperty GRAPH_ID , GRAPH_HOOKS_PATH , GRAPH_LIBRARY_PATH and GRAPH_LIBRARY_CONFIG have been removed. These config options have been removed in favour of providing a graphConfig JSON and using GRAPH_CONFIG_PATH instead. rest.service.v2.example.ExamplesFactory This class has been removed. It is replaced by uk.gov.gchq.gaffer.rest.factory.ExamplesFactory , which can be used instead. store.StoreProperties Store ID ( gaffer.store.id ) and related methods ( getId() + setId(String) ) have been removed. The ID is instead set in GraphLibrary when adding (with add ) the StoreProperties . See the Javadoc for GraphLibrary for more detail. store.Context The private constructor Context(final User user, final Map<String, Object> config, final String jobId) has been removed; along with the jobId(String) method. Use Context(final User user, final Map<String, Object> config) instead. This does not support supplying the Job ID, this will be set automatically. To get the Job ID use .getJobId() . store.schema.TypeDefinition The method getSerialiserClass() has been removed. Instead, use getSerialiser() with .getClass() and related methods. The method setSerialiserClass(String) has been removed. Instead, set the Serialiser directly using setSerialiser(Serialiser) . store.schema.Schema Schema ID ( gaffer.store.id ) and related methods have been removed. The ID is now defined in GraphLibrary when adding the schema. timestampProperty and related methods have been removed. Instead, this is specified by setting \"config\": {\"timestampProperty\": \"timestamp\"} (where \"timestamp\" is the property name to use as a time stamp) in the Schema. See this example schema for more info. The method getVertexSerialiserClass() has been removed. It can be replaced by calling vertexSerialiser.getClass() and converting the result as appropriate, e.g. getVertexSerialiserClass() used SimpleClassNameIdResolver.getSimpleClassName(vertexSerialiser.getClass()) . store.library.GraphLibrary The method addSchema(final Schema schema) has been removed. Use addSchema(final String id, final Schema schema) instead. The method addProperties(final StoreProperties properties) has been removed. Use addProperties(final String id, final StoreProperties properties) instead. Both of these now require the schema ID to be supplied. store.operation.OperationChainValidator The method validateViews(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store) has been removed. Use validateViews(final Operation op, final User user, final Store store, final ValidationResult validationResult) instead, passing user as null will result in the same behaviour. The method validateComparables(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store) has been removed. Use validateComparables(final Operation op, final User user, final Store store, final ValidationResult validationResult) instead, passing user as null will result in the same behaviour. store.operation.handler.named.cache.NamedViewCache The method deleteNamedView(final String name) has been removed. Use deleteNamedView(final String name, final User user) instead, passing user as null will result in the same behaviour. The method getNamedView(final String name) has been removed. Use getNamedView(final String name, final User user) instead. The method getAllNamedViews() has been removed. Use getAllNamedViews(final User user) instead. types.IntegerFreqMap This class has been removed. Use uk.gov.gchq.gaffer.types.FreqMap instead, this is identical except for using Long rather than Integer. types.function.IntegerFreqMapAggregator This class has been removed. Use uk.gov.gchq.gaffer.types.function.FreqMapAggregator instead. serialisation.IntegerFreqMapSerialiser This class has been removed. Use uk.gov.gchq.gaffer.serialisation.FreqMapSerialiser instead.","title":"Deprecations"},{"location":"deprecations/#deprecations","text":"This page describes deprecated code which has been removed in Gaffer 2 and how to migrate to better equivalents. Each heading for a section below refers to a classname from uk.gov.gchq.gaffer where there have been changes or where that class has been removed entirely. The section headings link to the code on GitHub for that class (as of the Gaffer 1.21.1 release). Deprecations impacting the serialisers used in schemas are listed first, followed by changes to Seed Matching and changes to Traits . Other deprecations are then listed in alphabetical order .","title":"Deprecations"},{"location":"deprecations/#serialisers","text":"","title":"Serialisers"},{"location":"deprecations/#migrating-away-from-deprecated-serialisers","text":"Various deprecated serialisers have been removed completely (details below). If any of these are being used in an existing schema, a new graph and schema will need to be created (see below for replacement serialisers to use) and data from existing graphs migrated. Data will need to be migrated (export and reimport) from graphs using deprecated serialisers before upgrading to Gaffer v2. It is essential to migrate data stored using deprecated serialisers. Simply replacing these serialisers is not enough because this will prevent existing data from being read and potentially put the backing store into a corrupted state .","title":"Migrating away from deprecated Serialisers"},{"location":"deprecations/#preservation-of-ordering","text":"When using an ordered store (such as Accumulo), all serialisers used on vertices must preserve order. As such, compactRaw serialisers (which do not preserve order) cannot be used on vertices in ordered stores. However, when preserving order is not required, such as for properties, compactRaw serialisers are the most effective solution and should always be used. Using an ordered serialiser on a property would reduce performance without providing any benefit. See the schemas documentation for more detail .","title":"Preservation of ordering"},{"location":"deprecations/#serialisationimplementationrawrawdateserialiser","text":"This class has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser instead - note that this will preserve order.","title":"serialisation.implementation.raw.RawDateSerialiser"},{"location":"deprecations/#serialisationdateserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDateSerialiser instead - note that this will preserve order. This doesn't implement .deserialiseString(String) , instead use new Date(Long.parseLong(value)) in place of this.","title":"serialisation.DateSerialiser"},{"location":"deprecations/#serialisationimplementationrawrawdoubleserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser instead - note that this will preserve order.","title":"serialisation.implementation.raw.RawDoubleSerialiser"},{"location":"deprecations/#serialisationdoubleserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedDoubleSerialiser instead - note that this will preserve order. This doesn't implement .deserialiseString(String) , instead use Double.parseDouble(value) in place of this.","title":"serialisation.DoubleSerialiser"},{"location":"deprecations/#serialisationimplementationrawrawfloatserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser instead - note that this will preserve order.","title":"serialisation.implementation.raw.RawFloatSerialiser"},{"location":"deprecations/#serialisationfloatserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedFloatSerialiser instead - note that this will preserve order.","title":"serialisation.FloatSerialiser"},{"location":"deprecations/#serialisationintegerserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser could also be used instead. Neither of these implement .deserialiseString(String) , instead use Integer.parseInt(value) in place of this.","title":"serialisation.IntegerSerialiser"},{"location":"deprecations/#serialisationimplementationrawrawintegerserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedIntegerSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawIntegerSerialiser should instead be used.","title":"serialisation.implementation.raw.RawIntegerSerialiser"},{"location":"deprecations/#serialisationlongserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser could also be used instead.","title":"serialisation.LongSerialiser"},{"location":"deprecations/#serialisationimplementationrawrawlongserialiser","text":"This has been removed. Use uk.gov.gchq.gaffer.serialisation.implementation.ordered.OrderedLongSerialiser instead, this will preserve order. If object ordering does not need to be preserved, uk.gov.gchq.gaffer.serialisation.implementation.raw.CompactRawLongSerialiser should instead be used.","title":"serialisation.implementation.raw.RawLongSerialiser"},{"location":"deprecations/#serialisationtobytesserialiser","text":"The method deserialise(byte[]) has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface. It is recommended for speed/performance to use the other implementation with an offset and a length - deserialise(byte[], int, int) .","title":"serialisation.ToBytesSerialiser"},{"location":"deprecations/#serialisationtobytesviastringdeserialiser","text":"The method deserialise(byte[]) has been marked as deprecated. It cannot be deleted as it is needed to implement the Serialiser interface. It is recommended for speed/performance to use the other implementation with an offset and a length - deserialise(byte[], int, int) .","title":"serialisation.ToBytesViaStringDeserialiser"},{"location":"deprecations/#removal-of-seed-matching","text":"","title":"Removal of Seed Matching"},{"location":"deprecations/#operationseedmatching","text":"This class has been removed. Use a View instead. See the Gaffer v1 docs for more detail on how to replace seed matching with a view.","title":"operation.SeedMatching"},{"location":"deprecations/#changes-to-store-traits","text":"","title":"Changes to Store Traits"},{"location":"deprecations/#storestore","text":"The method getTraits() has been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead. The method hasTrait(StoreTrait) has been removed. Use Store.execute(Operation, Context) with the HasTrait operation instead.","title":"store.Store"},{"location":"deprecations/#federatedstorefederatedgraphstorage","text":"The method getTraits(GetTraits, Context) has been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead.","title":"federatedstore.FederatedGraphStorage"},{"location":"deprecations/#federatedstorefederatedstore","text":"The methods getTraits() and getTraits(GetTraits, Context) have been removed. Use Store.execute(Operation, Context) with the GetTraits operation instead.","title":"federatedstore.FederatedStore"},{"location":"deprecations/#all-other-deprecations","text":"","title":"All other Deprecations"},{"location":"deprecations/#accumulostoreaccumuloproperties","text":"The TABLE setting/variable plus the methods getTable() and setTable(String) have been removed. For getTable() , uk.gov.gchq.gaffer.accumulostore.getTableName() could be used instead. A graphId should be supplied instead of setting TABLE directly.","title":"accumulostore.AccumuloProperties"},{"location":"deprecations/#accumulostoremockaccumulostore","text":"This class has been removed. For in memory graphs, use uk.gov.gchq.gaffer.mapstore.MapStore instead. For tests use uk.gov.gchq.gaffer.accumulostore.MiniAccumuloStore instead.","title":"accumulostore.MockAccumuloStore"},{"location":"deprecations/#commonutiltesttypes","text":"This class has been removed. Use the equivalent TestTypes class in the store module uk.gov.gchq.gaffer.store.TestTypes instead.","title":"commonutil.TestTypes"},{"location":"deprecations/#dataelementdefinitionviewnamedviewdetail","text":"The method hasWriteAccess(final String userId, final Set<String> opAuths, final String adminAuth) has been removed. Use hasWriteAccess(final User user, final String adminAuth) instead.","title":"data.elementdefinition.view.NamedViewDetail"},{"location":"deprecations/#dataelementdefinitionviewviewelementdefinition","text":"The method setAggregator(final ElementAggregator aggregator) has been removed. A ViewElementDefinition should be constructed using the builder uk.gov.gchq.gaffer.data.elementdefinition.view.ViewElementDefinition.Builder instead.","title":"data.elementdefinition.view.ViewElementDefinition"},{"location":"deprecations/#federatedstorefederatedaccess","text":"The method isAddingUser(User) has been removed. Use hasReadAccess(User user, String adminAuth) / hasWriteAccess(User user, String adminAuth) instead.","title":"federatedstore.FederatedAccess"},{"location":"deprecations/#federatedstorefederatedgraphstorage_1","text":"The methods getAllIdsAsAdmin() , getAllGraphAndAccessAsAdmin(List<String>) and changeGraphAccessAsAdmin(String, FederatedAccess) have all been removed. The method remove(String graphId) has been removed. The following can be used instead: remove(String graphId, User user) remove(String graphId, User user, String adminAuth) remove(String graphId, Predicate<Entry<FederatedAccess, Set<Graph>>> entryPredicateForGraphRemoval)","title":"federatedstore.FederatedGraphStorage"},{"location":"deprecations/#federatedstorefederatedstore_1","text":"The method updateOperationForGraph(Operation, Graph) has been removed. Use FederatedStoreUtil.updateOperationForGraph(Operation, Graph) instead. The method addGraphs(Set<String> graphAuths, String addingUserId, GraphSerialisable... graphs) has been removed. The following can be used instead: addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, GraphSerialisable... graphs) addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, GraphSerialisable... graphs) addGraphs(Set<String> graphAuths, String addingUserId, boolean isPublic, boolean disabledByDefault, AccessPredicate readAccessPredicate, AccessPredicate writeAccessPredicate, GraphSerialisable... graphs) addGraphs(FederatedAccess access, GraphSerialisable... graphs)","title":"federatedstore.FederatedStore"},{"location":"deprecations/#federatedstoreoperationremovegraph","text":"The method Builder.setGraphId(final String graphId) has been removed. Use Builder.graphId(final String graphId) which has identical behaviour instead.","title":"federatedstore.operation.RemoveGraph"},{"location":"deprecations/#graphgraph","text":"The methods Builder.graphId , Builder.library , Builder.view , Builder.addHook , Builder.addHooks have all been removed in all forms. Instead of using these methods, use .config() to set the graphConfig .","title":"graph.Graph"},{"location":"deprecations/#hdfsoperationmapreduce","text":"The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. Gaffer\u2019s operations that inherit MapReduce did not make use of numReduceTasks , either setting it to a constant number in the JobFactory or using Accumulo to automatically set the number (recommended for performance) and using min/max to keep it within a range. Therefore, numReduceTasks , getNumReduceTasks and setNumReduceTasks have been removed from this interface.","title":"hdfs.operation.MapReduce"},{"location":"deprecations/#hdfsoperationaddelementsfromhdfs","text":"The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. The number of reduce tasks should not be set. By default the number of reduce tasks should match the number of tablets. Use minimum and maximum reduce tasks to specify boundaries for the number of reduce tasks.","title":"hdfs.operation.AddElementsFromHdfs"},{"location":"deprecations/#hdfsoperationsampledataforsplitpoints","text":"The methods getNumReduceTasks() and setNumReduceTasks(Integer) have been removed. These methods were not required as NumReduceTasks was always set to 1 in any case.","title":"hdfs.operation.SampleDataForSplitPoints"},{"location":"deprecations/#jobtrackerjobdetail","text":"The constructors which took userId as a String have been removed. Instead, a User ( uk.gov.gchq.gaffer.user.User ) should be used in its place. See the Builder for User . getUserId and setUserId have also been removed. For getting the UserId , getUser().getUserId() can be used instead. See the Javadoc for User .","title":"jobtracker.JobDetail"},{"location":"deprecations/#jsonserialisationjsonserialiser","text":"The method update(final String jsonSerialiserClass, final String jsonSerialiserModules) has been removed. Use update(final String jsonSerialiserClass, final String jsonSerialiserModules, final Boolean strictJson) instead. Passing strictJson as null will result in the same behaviour.","title":"jsonserialisation.JSONSerialiser"},{"location":"deprecations/#operationoperation","text":"The method asOperationChain(final Operation operation) has been removed. Use OperationChain.wrap with the Operation instead.","title":"operation.Operation"},{"location":"deprecations/#operationimplgetwalks","text":"The method Builder.operation has been removed. Use the vararg method Builder.addOperations instead.","title":"operation.impl.GetWalks"},{"location":"deprecations/#operationimplsplitstore","text":"This class has been removed. It is replaced by SplitStoreFromFile which is identical except in name.","title":"operation.impl.SplitStore"},{"location":"deprecations/#operationimpljoinmethodsjoinfunction","text":"The method join(final Iterable keys, final String keyName, final String matchingValuesName, final Match match, final Boolean flatten) which was not implemented has been removed.","title":"operation.impl.join.methods.JoinFunction"},{"location":"deprecations/#restsystemproperty","text":"GRAPH_ID , GRAPH_HOOKS_PATH , GRAPH_LIBRARY_PATH and GRAPH_LIBRARY_CONFIG have been removed. These config options have been removed in favour of providing a graphConfig JSON and using GRAPH_CONFIG_PATH instead.","title":"rest.SystemProperty"},{"location":"deprecations/#restservicev2exampleexamplesfactory","text":"This class has been removed. It is replaced by uk.gov.gchq.gaffer.rest.factory.ExamplesFactory , which can be used instead.","title":"rest.service.v2.example.ExamplesFactory"},{"location":"deprecations/#storestoreproperties","text":"Store ID ( gaffer.store.id ) and related methods ( getId() + setId(String) ) have been removed. The ID is instead set in GraphLibrary when adding (with add ) the StoreProperties . See the Javadoc for GraphLibrary for more detail.","title":"store.StoreProperties"},{"location":"deprecations/#storecontext","text":"The private constructor Context(final User user, final Map<String, Object> config, final String jobId) has been removed; along with the jobId(String) method. Use Context(final User user, final Map<String, Object> config) instead. This does not support supplying the Job ID, this will be set automatically. To get the Job ID use .getJobId() .","title":"store.Context"},{"location":"deprecations/#storeschematypedefinition","text":"The method getSerialiserClass() has been removed. Instead, use getSerialiser() with .getClass() and related methods. The method setSerialiserClass(String) has been removed. Instead, set the Serialiser directly using setSerialiser(Serialiser) .","title":"store.schema.TypeDefinition"},{"location":"deprecations/#storeschemaschema","text":"Schema ID ( gaffer.store.id ) and related methods have been removed. The ID is now defined in GraphLibrary when adding the schema. timestampProperty and related methods have been removed. Instead, this is specified by setting \"config\": {\"timestampProperty\": \"timestamp\"} (where \"timestamp\" is the property name to use as a time stamp) in the Schema. See this example schema for more info. The method getVertexSerialiserClass() has been removed. It can be replaced by calling vertexSerialiser.getClass() and converting the result as appropriate, e.g. getVertexSerialiserClass() used SimpleClassNameIdResolver.getSimpleClassName(vertexSerialiser.getClass()) .","title":"store.schema.Schema"},{"location":"deprecations/#storelibrarygraphlibrary","text":"The method addSchema(final Schema schema) has been removed. Use addSchema(final String id, final Schema schema) instead. The method addProperties(final StoreProperties properties) has been removed. Use addProperties(final String id, final StoreProperties properties) instead. Both of these now require the schema ID to be supplied.","title":"store.library.GraphLibrary"},{"location":"deprecations/#storeoperationoperationchainvalidator","text":"The method validateViews(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store) has been removed. Use validateViews(final Operation op, final User user, final Store store, final ValidationResult validationResult) instead, passing user as null will result in the same behaviour. The method validateComparables(final Operation op, final ValidationResult validationResult, final Schema schemaNotUsed, final Store store) has been removed. Use validateComparables(final Operation op, final User user, final Store store, final ValidationResult validationResult) instead, passing user as null will result in the same behaviour.","title":"store.operation.OperationChainValidator"},{"location":"deprecations/#storeoperationhandlernamedcachenamedviewcache","text":"The method deleteNamedView(final String name) has been removed. Use deleteNamedView(final String name, final User user) instead, passing user as null will result in the same behaviour. The method getNamedView(final String name) has been removed. Use getNamedView(final String name, final User user) instead. The method getAllNamedViews() has been removed. Use getAllNamedViews(final User user) instead.","title":"store.operation.handler.named.cache.NamedViewCache"},{"location":"deprecations/#typesintegerfreqmap","text":"This class has been removed. Use uk.gov.gchq.gaffer.types.FreqMap instead, this is identical except for using Long rather than Integer.","title":"types.IntegerFreqMap"},{"location":"deprecations/#typesfunctionintegerfreqmapaggregator","text":"This class has been removed. Use uk.gov.gchq.gaffer.types.function.FreqMapAggregator instead.","title":"types.function.IntegerFreqMapAggregator"},{"location":"deprecations/#serialisationintegerfreqmapserialiser","text":"This class has been removed. Use uk.gov.gchq.gaffer.serialisation.FreqMapSerialiser instead.","title":"serialisation.IntegerFreqMapSerialiser"},{"location":"log4j/","text":"Log4j in Gaffer This page contains information on how logging is done in Gaffer and on previous use of Log4j in Gaffer. Log4j Version Log4j version 1 (1.2.17), was used by Gaffer versions 1.21 and below. From Gaffer 1.22, Log4j was replaced with Reload4j. The newer version of Log4j, Log4j2 - which is susceptible to the major Log4Shell attack, has never been used by Gaffer or its dependencies. How Logging is done Gaffer uses SLF4J ( Simple Logging Facade for Java ) for all logging. This is a framework/abstraction layer which allows for different loggers to be used ( known as bindings ). The binding used by Gaffer is org.slf4j:slf4j-reload4j:jar:1.7.36 . Impact of Log4j removal on projects incorporating Gaffer Gaffer now uses Reload4j via SLF4J. This may impact projects which are using Gaffer if they are using Log4j directly or through a transitive dependency. To help avoid dependency conflicts, we have configured maven-enforcer-plugin to block use of Log4j with Gaffer. If you are using Gaffer in your project and your build fails because of this plugin, you will need to add a dependency exclusion to any dependencies which depend transitively on Log4j. These can be found by using the Maven dependency tree (ideally in verbose mode). Dependencies of Gaffer using Log4j 1.2.17 Some major Gaffer dependencies (listed below) use Log4j internally (either directly or through SLF4J). From Gaffer version 1.22 these transitive dependencies are excluded and replaced with Reload4j, such that Log4j does not appear on the classpath at all. GCHQ Koryphe 1.14.0 - Uses SLF4J with Log4j. Apache HBase 1.3.0 - Multiple artefacts used from the group org.apache.hbase . All depend directly on Log4j. Apache Hadoop 2.6.5 - Multiple artefacts used from the group org.apache.hadoop . All depend directly on Log4j. Apache Accumulo 1.9.3 - Multiple artefacts used from the group org.apache.accumulo . All depend directly on Log4j. Apache Kafka 0.10.0.0 - Artefact depends indirectly on Log4j through a sub dependency ( com.101tec:zkclient ). Apache Spark 2.3.2 - Artefact depends directly on Log4j. Log4j Vulnerabilities Current vulnerabilities in Log4j 1.12.17 relate to the JDBC, SMTP and JMS appenders, the JMS Sink and the Socket Server. Gaffer never used any of this. In its default configuration, we don't believe Gaffer is vulnerable to any of these problems. If the Log4j configuration is altered, changes could be made which may cause Gaffer to be vulnerable to one or more of the above vulnerabilities. Standard security processes to prevent unauthorised access and modification of configuration files should preclude this possibility.","title":"Log4j in Gaffer"},{"location":"log4j/#log4j-in-gaffer","text":"This page contains information on how logging is done in Gaffer and on previous use of Log4j in Gaffer.","title":"Log4j in Gaffer"},{"location":"log4j/#log4j-version","text":"Log4j version 1 (1.2.17), was used by Gaffer versions 1.21 and below. From Gaffer 1.22, Log4j was replaced with Reload4j. The newer version of Log4j, Log4j2 - which is susceptible to the major Log4Shell attack, has never been used by Gaffer or its dependencies.","title":"Log4j Version"},{"location":"log4j/#how-logging-is-done","text":"Gaffer uses SLF4J ( Simple Logging Facade for Java ) for all logging. This is a framework/abstraction layer which allows for different loggers to be used ( known as bindings ). The binding used by Gaffer is org.slf4j:slf4j-reload4j:jar:1.7.36 .","title":"How Logging is done"},{"location":"log4j/#impact-of-log4j-removal-on-projects-incorporating-gaffer","text":"Gaffer now uses Reload4j via SLF4J. This may impact projects which are using Gaffer if they are using Log4j directly or through a transitive dependency. To help avoid dependency conflicts, we have configured maven-enforcer-plugin to block use of Log4j with Gaffer. If you are using Gaffer in your project and your build fails because of this plugin, you will need to add a dependency exclusion to any dependencies which depend transitively on Log4j. These can be found by using the Maven dependency tree (ideally in verbose mode).","title":"Impact of Log4j removal on projects incorporating Gaffer"},{"location":"log4j/#dependencies-of-gaffer-using-log4j-1217","text":"Some major Gaffer dependencies (listed below) use Log4j internally (either directly or through SLF4J). From Gaffer version 1.22 these transitive dependencies are excluded and replaced with Reload4j, such that Log4j does not appear on the classpath at all. GCHQ Koryphe 1.14.0 - Uses SLF4J with Log4j. Apache HBase 1.3.0 - Multiple artefacts used from the group org.apache.hbase . All depend directly on Log4j. Apache Hadoop 2.6.5 - Multiple artefacts used from the group org.apache.hadoop . All depend directly on Log4j. Apache Accumulo 1.9.3 - Multiple artefacts used from the group org.apache.accumulo . All depend directly on Log4j. Apache Kafka 0.10.0.0 - Artefact depends indirectly on Log4j through a sub dependency ( com.101tec:zkclient ). Apache Spark 2.3.2 - Artefact depends directly on Log4j.","title":"Dependencies of Gaffer using Log4j 1.2.17"},{"location":"log4j/#log4j-vulnerabilities","text":"Current vulnerabilities in Log4j 1.12.17 relate to the JDBC, SMTP and JMS appenders, the JMS Sink and the Socket Server. Gaffer never used any of this. In its default configuration, we don't believe Gaffer is vulnerable to any of these problems. If the Log4j configuration is altered, changes could be made which may cause Gaffer to be vulnerable to one or more of the above vulnerabilities. Standard security processes to prevent unauthorised access and modification of configuration files should preclude this possibility.","title":"Log4j Vulnerabilities"},{"location":"openCypher/","text":"openCypher Data Format openCypher was chosen as it represents \u201cthe most widely adopted, fully-specified, and open query language for property graph databases\u201d. Allowing customers to import data from enterprise applications such as neo4j and Amazon Neptune with greater ease. One caveat to this is that there are slight differences in the System column header formats used by each, these differences are shown below. For more info regarding the data format see here Entity (Node) System column headers Neptune / neo4j :ID / _id \u2013 (Required) An ID for the node -> VERTEX (Gaffer) :LABEL / _labels \u2013 A label for the node -> GROUP (Gaffer) Edge (relationship) System column headers Neptune / neo4j :ID / _id \u2013 An ID for the relationship. There doesn't exist a direct mapping between \"ID\" in openCypher and a dedicated attribute for an Edge with Gaffer, instead this is added as a property belonging to the Edge. :START_ID / _start \u2013 (Required) The node ID of the node this relationship starts from -> SOURCE (Gaffer) :END_ID / _end \u2013 (Required) The node ID of the node this relationship ends at -> DESTINATION (Gaffer) :TYPE / _type \u2013 A type for the relationship -> GROUP (Gaffer) Property column headers Gaffer replaces \"-\" with \"_\" from column headers whilst other non valid characters as outlined within the class PropertiesUtil are stripped. Both Entities and Edges can have associated properties. It's possible to specify the type of each property provided by using the proceeding format propertyname:type , the default is to treat each property as a string if a type isn't supplied. Supported Data Types and How Gaffer Handles Each Serialised to Boolean Bool or Boolean \u2013 A Boolean field. Allowed values are true and false. Any value other than true is treated as false. Serialised to Integer Byte \u2013 A whole number in the range -128 through 127. Converted to an integer value. Short \u2013 A whole number in the range -32,768 through 32,767. Converted to an integer value. Int \u2013 A whole number in the range -2^31 through 2^31 - 1. Serialised to Long Long \u2013 A whole number in the range -2^63 through 2^63 - 1. Serialised to Float Float \u2013 A 32-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not. Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level). Serialised to Double Double \u2013 A 64-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not. Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level). Serialised to TimeStamp DateTime \u2013 A Java date in one of the following ISO-8601 formats: yyyy-MM-dd yyyy-MM-ddTHH:mm yyyy-MM-ddTHH:mm:ss yyyy-MM-ddTHH:mm:ssZ Serialised to String String \u2013 Quotation marks are optional. Comma, newline, and carriage return characters are automatically escaped if they are included in a string that is surrounded by double quotation marks (\"). Char \u2013 A Char field. Stored as a string. Date, LocalDate, and LocalDateTime, \u2013 See Neo4j Temporal Instants for a description of the date, localdate, and localdatetime types. The values are loaded verbatim as strings, without validation. Duration \u2013 See the Neo4j Duration format. The values are loaded verbatim as strings, without validation. Point \u2013 A point field, for storing spatial data. See Neo4j Point format. The values are loaded verbatim as strings, without validation. Example of the openCypher load format :ID, name:String, age:Int, lang:String, :LABEL, :START_ID, :END_ID, :TYPE, weight:Double v1, \"marko\", 29, , person, , , , v2, \"lop\", , \"java\",software, , , , e1, , , , , v1, v2, created, 0.4","title":"openCypher Data Format"},{"location":"openCypher/#opencypher-data-format","text":"openCypher was chosen as it represents \u201cthe most widely adopted, fully-specified, and open query language for property graph databases\u201d. Allowing customers to import data from enterprise applications such as neo4j and Amazon Neptune with greater ease. One caveat to this is that there are slight differences in the System column header formats used by each, these differences are shown below. For more info regarding the data format see here","title":"openCypher Data Format"},{"location":"openCypher/#entity-node-system-column-headers","text":"Neptune / neo4j :ID / _id \u2013 (Required) An ID for the node -> VERTEX (Gaffer) :LABEL / _labels \u2013 A label for the node -> GROUP (Gaffer)","title":"Entity (Node) System column headers"},{"location":"openCypher/#edge-relationship-system-column-headers","text":"Neptune / neo4j :ID / _id \u2013 An ID for the relationship. There doesn't exist a direct mapping between \"ID\" in openCypher and a dedicated attribute for an Edge with Gaffer, instead this is added as a property belonging to the Edge. :START_ID / _start \u2013 (Required) The node ID of the node this relationship starts from -> SOURCE (Gaffer) :END_ID / _end \u2013 (Required) The node ID of the node this relationship ends at -> DESTINATION (Gaffer) :TYPE / _type \u2013 A type for the relationship -> GROUP (Gaffer)","title":"Edge (relationship) System column headers"},{"location":"openCypher/#property-column-headers","text":"Gaffer replaces \"-\" with \"_\" from column headers whilst other non valid characters as outlined within the class PropertiesUtil are stripped. Both Entities and Edges can have associated properties. It's possible to specify the type of each property provided by using the proceeding format propertyname:type , the default is to treat each property as a string if a type isn't supplied.","title":"Property column headers"},{"location":"openCypher/#supported-data-types-and-how-gaffer-handles-each","text":"","title":"Supported Data Types and How Gaffer Handles Each"},{"location":"openCypher/#serialised-to-boolean","text":"Bool or Boolean \u2013 A Boolean field. Allowed values are true and false. Any value other than true is treated as false.","title":"Serialised to Boolean"},{"location":"openCypher/#serialised-to-integer","text":"Byte \u2013 A whole number in the range -128 through 127. Converted to an integer value. Short \u2013 A whole number in the range -32,768 through 32,767. Converted to an integer value. Int \u2013 A whole number in the range -2^31 through 2^31 - 1.","title":"Serialised to Integer"},{"location":"openCypher/#serialised-to-long","text":"Long \u2013 A whole number in the range -2^63 through 2^63 - 1.","title":"Serialised to Long"},{"location":"openCypher/#serialised-to-float","text":"Float \u2013 A 32-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not. Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level).","title":"Serialised to Float"},{"location":"openCypher/#serialised-to-double","text":"Double \u2013 A 64-bit IEEE 754 floating point number. Decimal notation and scientific notation are both supported. Infinity, -Infinity, and NaN are all recognized, but INF is not. Values with too many digits to fit are rounded to the nearest value (a midway value is rounded to 0 for the last remaining digit at the bit level).","title":"Serialised to Double"},{"location":"openCypher/#serialised-to-timestamp","text":"DateTime \u2013 A Java date in one of the following ISO-8601 formats: yyyy-MM-dd yyyy-MM-ddTHH:mm yyyy-MM-ddTHH:mm:ss yyyy-MM-ddTHH:mm:ssZ","title":"Serialised to TimeStamp"},{"location":"openCypher/#serialised-to-string","text":"String \u2013 Quotation marks are optional. Comma, newline, and carriage return characters are automatically escaped if they are included in a string that is surrounded by double quotation marks (\"). Char \u2013 A Char field. Stored as a string. Date, LocalDate, and LocalDateTime, \u2013 See Neo4j Temporal Instants for a description of the date, localdate, and localdatetime types. The values are loaded verbatim as strings, without validation. Duration \u2013 See the Neo4j Duration format. The values are loaded verbatim as strings, without validation. Point \u2013 A point field, for storing spatial data. See Neo4j Point format. The values are loaded verbatim as strings, without validation.","title":"Serialised to String"},{"location":"openCypher/#example-of-the-opencypher-load-format","text":":ID, name:String, age:Int, lang:String, :LABEL, :START_ID, :END_ID, :TYPE, weight:Double v1, \"marko\", 29, , person, , , , v2, \"lop\", , \"java\",software, , , , e1, , , , , v1, v2, created, 0.4","title":"Example of the openCypher load format"},{"location":"ways-of-working/","text":"Ways of Working Git branching model We have adopted the GitFlow Branching Model in order to support both Gaffer v1 and v2: Issues Where possible a pull request should correlate to a single GitHub issue. An issue should relate to a single functional or non-functional change - changes to alter/improve other pieces of functionality should be addressed in a separate issue in order to keep reviews atomic. The reasoning behind code changes should be documented in the GitHub issue. All resolved issues should be included in the next GitHub milestone, this enables releases to be linked to the included issues. If a code change requires users of Gaffer to make changes in order for them to adopt it then the issue should be labelled 'migration-required' and a comment should be added similar to: ### Migration Steps [Description of what needs to be done to adopt the code change with examples] Workflow Assign yourself to the issue Create a new branch off develop using pattern: gh-[issue number]-[issue-title] Commit your changes using descriptive commit titles Check and push your changes Create a pull request (PR) to merge your branch into develop , prefixing the PR title with \"Gh-[issue number]: \" If you named the branch and PR correctly, the PR should have \"Resolve #[issue-number]\" automatically added to the description after it is made. If it doesn't, then please add the issue it will resolve as a \"Linked issue\" If there is a significant change, please follow the same process to document the change in gaffer-doc The pull request will be reviewed and following any changes and approval your branch will be squashed and merged into develop Delete the branch The issue will be closed automatically Pull Requests Pull requests will undergo a review by a Gaffer committer to check the code changes are compliant with our coding style. This is a community so please be respectful of other members - offer encouragement, support and suggestions. As described in our git branching model - please raise pull requests to merge your changes in our develop branch. When pull requests are accepted, the reviewer should squash and merge them. This is because it keeps the develop branch clean and populated with only merge commits, rather than intermediate ones. As well as this, it makes everyone's job reviewing pull requests easier as any insecure and unreviewed intermediate commits are not included into the develop branch. Please agree to the GCHQ OSS Contributor License Agreement before submitting a pull request. Signing the CLA is enforced by the cla-assistant. Documentation As mentioned before, any significant changes in a PR should be accompanied with an addition to Gaffer's documentation: gaffer-doc . Smaller changes should be self documented in the tests. With this approach, any large feature or change has user friendly documentation, whereas technical or implementation details are documented for developers by the tests. Coding style Java Please ensure your coding style is consistent with the rest of the Gaffer project and the Google Java Style Guide . Your changes should pass the checkstyle and spotless plugins that are part of the continuous integration pipeline and check for code formatting and licenses. Before you push your changes you can check the checkstyle plugin passes with mvn checkstyle:check and check the spotless plugin passes with mvn spotless:check . Python Please ensure your coding style is consistent with the rest of the Gaffer project and the PEP 8 Style Guide . However, there are a few exceptions to the standards set by PEP8: * Module level imports at the top of the file - this will not be enforced but is recommended where it does not cause issues with the code generated by Fishbowl. * Max line length of 79 characters - the max line length that will be enforced in this project has been increased to 100 characters. Before you create a PR for your changes you can use autopep8 to check and fix any styling issues. The following can be run which will take into account the rule exceptions mentioned above. autopep8 --exit-code -r -i -a -a --max-line-length 100 --ignore E402 . Javadoc Ensure your java code has sufficient javadocs explaining what the section of code does and the intended use of it. Javadocs should be used in addition to clean readable code. In particular: * All public classes (not required for test classes unless an explanation of the testing is required) * public methods (not required if the functionality is obvious from the method name) * public constants (not required if the constant is obvious from the name) Tests All new code should be unit tested. Where this is not possible the code should be invoked and the functionality should be tested in an integration test. In a small number of cases this will not be possible - instead steps to verify the code should be thoroughly documented. Tests should cover edge cases and exception cases as well as normal expected behavior. Keep each test decoupled and don't rely on tests running in a given order - don't save state between tests. For a given code change, aim to improve the code coverage. Unit test classes should test a single class and be named [testClass]Test. Integration test classes should be named [functionalityUnderTest]IT. Tests should be readable and self documenting. Each test should focus on testing one small piece of functionality invoked from a single method call. Tests should use JUnit 5 and assertJ. We suggest the following pattern: @Test public void should [ DoSomething | ReturnSomething ] { // Given [ Setup your test here ] // When [ Invoke the test method ] // Then [ assertThat the method did what was expected ] } Gaffer 2 During the Gaffer 2 development process there is a v2-alpha branch, which acts as the develop branch for changes that will only be added to Gaffer 2.","title":"Ways of Working"},{"location":"ways-of-working/#ways-of-working","text":"","title":"Ways of Working"},{"location":"ways-of-working/#git-branching-model","text":"We have adopted the GitFlow Branching Model in order to support both Gaffer v1 and v2:","title":"Git branching model"},{"location":"ways-of-working/#issues","text":"Where possible a pull request should correlate to a single GitHub issue. An issue should relate to a single functional or non-functional change - changes to alter/improve other pieces of functionality should be addressed in a separate issue in order to keep reviews atomic. The reasoning behind code changes should be documented in the GitHub issue. All resolved issues should be included in the next GitHub milestone, this enables releases to be linked to the included issues. If a code change requires users of Gaffer to make changes in order for them to adopt it then the issue should be labelled 'migration-required' and a comment should be added similar to: ### Migration Steps [Description of what needs to be done to adopt the code change with examples]","title":"Issues"},{"location":"ways-of-working/#workflow","text":"Assign yourself to the issue Create a new branch off develop using pattern: gh-[issue number]-[issue-title] Commit your changes using descriptive commit titles Check and push your changes Create a pull request (PR) to merge your branch into develop , prefixing the PR title with \"Gh-[issue number]: \" If you named the branch and PR correctly, the PR should have \"Resolve #[issue-number]\" automatically added to the description after it is made. If it doesn't, then please add the issue it will resolve as a \"Linked issue\" If there is a significant change, please follow the same process to document the change in gaffer-doc The pull request will be reviewed and following any changes and approval your branch will be squashed and merged into develop Delete the branch The issue will be closed automatically","title":"Workflow"},{"location":"ways-of-working/#pull-requests","text":"Pull requests will undergo a review by a Gaffer committer to check the code changes are compliant with our coding style. This is a community so please be respectful of other members - offer encouragement, support and suggestions. As described in our git branching model - please raise pull requests to merge your changes in our develop branch. When pull requests are accepted, the reviewer should squash and merge them. This is because it keeps the develop branch clean and populated with only merge commits, rather than intermediate ones. As well as this, it makes everyone's job reviewing pull requests easier as any insecure and unreviewed intermediate commits are not included into the develop branch. Please agree to the GCHQ OSS Contributor License Agreement before submitting a pull request. Signing the CLA is enforced by the cla-assistant.","title":"Pull Requests"},{"location":"ways-of-working/#documentation","text":"As mentioned before, any significant changes in a PR should be accompanied with an addition to Gaffer's documentation: gaffer-doc . Smaller changes should be self documented in the tests. With this approach, any large feature or change has user friendly documentation, whereas technical or implementation details are documented for developers by the tests.","title":"Documentation"},{"location":"ways-of-working/#coding-style","text":"","title":"Coding style"},{"location":"ways-of-working/#java","text":"Please ensure your coding style is consistent with the rest of the Gaffer project and the Google Java Style Guide . Your changes should pass the checkstyle and spotless plugins that are part of the continuous integration pipeline and check for code formatting and licenses. Before you push your changes you can check the checkstyle plugin passes with mvn checkstyle:check and check the spotless plugin passes with mvn spotless:check .","title":"Java"},{"location":"ways-of-working/#python","text":"Please ensure your coding style is consistent with the rest of the Gaffer project and the PEP 8 Style Guide . However, there are a few exceptions to the standards set by PEP8: * Module level imports at the top of the file - this will not be enforced but is recommended where it does not cause issues with the code generated by Fishbowl. * Max line length of 79 characters - the max line length that will be enforced in this project has been increased to 100 characters. Before you create a PR for your changes you can use autopep8 to check and fix any styling issues. The following can be run which will take into account the rule exceptions mentioned above. autopep8 --exit-code -r -i -a -a --max-line-length 100 --ignore E402 .","title":"Python"},{"location":"ways-of-working/#javadoc","text":"Ensure your java code has sufficient javadocs explaining what the section of code does and the intended use of it. Javadocs should be used in addition to clean readable code. In particular: * All public classes (not required for test classes unless an explanation of the testing is required) * public methods (not required if the functionality is obvious from the method name) * public constants (not required if the constant is obvious from the name)","title":"Javadoc"},{"location":"ways-of-working/#tests","text":"All new code should be unit tested. Where this is not possible the code should be invoked and the functionality should be tested in an integration test. In a small number of cases this will not be possible - instead steps to verify the code should be thoroughly documented. Tests should cover edge cases and exception cases as well as normal expected behavior. Keep each test decoupled and don't rely on tests running in a given order - don't save state between tests. For a given code change, aim to improve the code coverage. Unit test classes should test a single class and be named [testClass]Test. Integration test classes should be named [functionalityUnderTest]IT. Tests should be readable and self documenting. Each test should focus on testing one small piece of functionality invoked from a single method call. Tests should use JUnit 5 and assertJ. We suggest the following pattern: @Test public void should [ DoSomething | ReturnSomething ] { // Given [ Setup your test here ] // When [ Invoke the test method ] // Then [ assertThat the method did what was expected ] }","title":"Tests"},{"location":"ways-of-working/#gaffer-2","text":"During the Gaffer 2 development process there is a v2-alpha branch, which acts as the develop branch for changes that will only be added to Gaffer 2.","title":"Gaffer 2"}]}